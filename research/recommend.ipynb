{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome\n"
     ]
    }
   ],
   "source": [
    "print(\"welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arshi\\Downloads\\Desktop\\Bro-Project\\SkillForge.ai\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"job_vector_db\",  # Path to saved folder\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.docstore._dict.values()\n",
    "\n",
    "# Convert to list if needed\n",
    "documents = list(docs)\n",
    "texts = ''\n",
    "for doc in documents:\n",
    "    texts += doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = doc_to_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role Description\n",
      " We are seeking a skilled\n",
      " Data Analyst \n",
      " with 4\\-6 years of experience to join our team. The ideal candidate will have strong expertise in Data analysis,\n",
      " SQL \n",
      " , Data Visualization coupled with excellent analytical and data validation skills. A solid grasp of statistical techniques and the ability to extract actionable insights from complex datasets is essential.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "Location \\- Pune or Kolkata\n",
      "Key Responsibilities \n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "* Data Pipeline Development:\n",
      "+ Design and implement robust data pipelines for data ingestion, cleansing, and transformation.\n",
      "\n",
      "* Data Extraction and Reporting:\n",
      "+ Extract relevant information from multiple data sources and create detailed reports to support stakeholder decision\\-making.\n",
      "\n",
      "* Cross\\-Functional Collaboration:\n",
      "+ Collaborate with cross\\-functional teams to understand business goals and translate them into analytical solutions.\n",
      "\n",
      "* Statistical Analysis:\n",
      "+ Utilize statistical techniques to analyze data, identifying patterns and trends for strategic decision\\-making.\n",
      "\n",
      "* Exploratory Data Analysis (EDA):\n",
      "+ Perform exploratory data analysis to uncover insights and generate hypotheses for further investigation.\n",
      "\n",
      "* Data Visualization:\n",
      "+ Develop and present clear, impactful visualizations and reports to communicate findings and recommendations effectively.\n",
      "\n",
      "* Management Reporting:\n",
      "+ Create comprehensive management reports that highlight trends, patterns, and actionable predictions based on data.\n",
      "\n",
      "Primary Skills\n",
      "* Good knowledge in Python or PySpark.\n",
      "* Strong SQL skills for data analysis and troubleshooting.\n",
      "* Advanced data analysis and validation expertise.\n",
      "* Proficiency in creating clear and actionable data visualizations.\n",
      "\n",
      "Secondary Skills (Good To Have)\n",
      "* Experience with MS Excel.\n",
      "* Familiarity with Databricks for advanced data processing and collaboration.\n",
      "\n",
      "Qualifications\n",
      "* 4\\-6 years of experience in data analysis or a related field.\n",
      "* Strong problem\\-solving and troubleshooting skills using SQL.\n",
      "\n",
      "Skills\n",
      " SQL Data Analysis Data Visualization, Python or PySparkJob Description\n",
      " Be part of the solution at Technip Energies and embark on a one\\-of\\-a\\-kind journey. You will be helping to develop cutting\\-edge solutions to solve real\\-world energy problems.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " We are currently seeking a Climate Data Analyst, reporting directly to\n",
      " *\\[reporting line position]* \n",
      " to join our team based in Noida.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " The Climate Data Analyst will be responsible for collecting, analyzing, and reporting data related to the company’s Greenhouse Gas emissions (scope 3\\). They will play a crucial role in supporting the development and implementation of Climate Transition strategies and action plans by providing insights derived from data analysis.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "About us:\n",
      " Technip Energies is a global technology and engineering powerhouse. With leadership positions in LNG, hydrogen, ethylene, sustainable chemistry, and CO2 management, we are contributing to the development of critical markets such as energy, energy derivatives, decarbonization, and circularity. Our complementary business segments, Technology, Products and Services (TPS) and Project Delivery, turn innovation into scalable and industrial reality.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " Through collaboration and excellence in execution, our 17,000\\+ employees across 34 countries are fully committed to bridging prosperity with sustainability for a world designed to last.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "About the opportunity we offer:\n",
      " The Climate Data Analyst will be responsible for collecting, analyzing, and reporting data related to the company’s Greenhouse Gas emissions (scope 3\\). They will play a crucial role in supporting the development and implementation of Climate Transition strategies and action plans by providing insights derived from data analysis.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "Job Accountabilities:\n",
      "* Carbon calculation (for scope 3 emissions)\n",
      "* Maintain and improve Carbon calculation tools\n",
      "* Ensure data accuracy, consistency, and integrity through regular audits as part of Data Quality run process.\n",
      "* Train and support projects team in calculating the carbon footprint on projects\n",
      "* Maintain Emission factors to be up\\-to\\-date\n",
      "* Ensure proper collection of the data from carbon calculation\n",
      "* Analyze data to identify trends, patterns, and insights related to GHG emissions reductions.\n",
      "* Prepare reports, dashboards and presentations to communicate GHG emissions performance.\n",
      "* Climate scenario analysis\n",
      "* Improve the Climate scenario analysis tool\n",
      "* Participate in elaborating different scenarios of GHG emissions reduction\n",
      "* Collaborate with finance modeling to ensure the alignment of data forecast\n",
      "* Elaborate a tool to monitor GHG emissions reduction\n",
      "* Data Analysis\n",
      "* Data crunching and competitive analysis for the various Climate\\-related topics.\n",
      "* Collect and manage climate\\-related data from various internal and external sources.\n",
      "* Support the Climate change team to optimize internal processes by using digital tools and artificial intelligence.\n",
      "* Gather data and develop a strategic competitor analysis dashboard and reports.\n",
      "* Collaborate with cross\\-functional teams, especially IDS and Digital to identify opportunities and develop solutions for the sustainability department.\n",
      "* Contribute to the continuous improvement of sustainability data management processes and systems.\n",
      "\n",
      "About you:\n",
      " Bachelor degree or superior in engineering and/or data sciences\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "* 4\\+ years of relevant work experience\n",
      "* Mastery of Microsoft Tools (PowerPoint, Word, Excel)\n",
      "* Hands\\-on experience with data visualization tools (PowerBI preferably)\n",
      "* Knowledge of Python programming and databases\n",
      "* Knowledge of artificial intelligence and machine learning\n",
      "* Excellent data analysis and organizational skills\n",
      "* Strong analytical and data management skills\n",
      "* Ability to work independently and manage multiple projects simultaneously\n",
      "* Knowledge of sustainability issues and commitment to driving sustainable change is a differentiator\n",
      "* Comfortable with complex ecosystem, moving environment and multi\\-stakeholders’ management\n",
      "* Good communication and interpersonal skills to effectively collaborate with various teams\n",
      "* Fluent English level is required with good writing skills\n",
      "\n",
      "What’s next?\n",
      " Once receiving your application, our Talent Acquisition professionals will screen and match your profile against the role requirements. We ask for your patience as the team completes the volume of applications with reasonable timeframe. Check your application progress periodically via personal account from created candidate profile during your application.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " We invite you to get to know more about our company by visiting and follow us on LinkedIn, Instagram, Facebook, X and YouTube for company updates.Hyderabad, preferred or remote\n",
      "About the Team\n",
      " DoorDash Labs is an independent team within DoorDash. We explore robotics and automation to transform last mile logistics in the long term. If you want to work on commercializing autonomy and robotics in a service used by millions of people, then we want to talk to you!\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "About the Role\n",
      " Come help us redefine last\\-mile logistics through robotics, automation, and other advanced technologies! DoorDash Labs is an independent team inside of DoorDash incubating long\\-term bets and exploring new technologies that can be applied to transform last\\-mile logistics. We believe in the value of advanced technologies and strive to build meaningful applications of next\\-generation hardware and software to impact real\\-world needs.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " We're looking for a data analyst to help bring autonomous robots and other high\\-tech automation products to market, at scale.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "You're excited about this opportunity because you will…\n",
      "* Own analytics and insights across multiple technical and product areas, ensuring data is structured to support clear measurement of business and product performance.\n",
      "* Define a strategy for commercial, operations, and autonomy insights to drive product improvements and answer critical business questions.\n",
      "* Report in directly with the product management team, and support a world\\-class team of engineers and business leaders to bring autonomous solutions to market\n",
      "* Use your skill in understanding and synthesis of data to create high value visualization and dashboards\n",
      "* Act as the primary data analytics partner for stakeholders across engineering, product, and leadership, helping to translate complex business questions into measurable data solutions.\n",
      "\n",
      "We're excited about you because…\n",
      "* You have a Bachelor's degree in business administration, economics, computer science, management information systems, or related field or equivalent related experience\n",
      "* You have 3\\-6 years of experience in data analytics or business analytics\n",
      "* You have experience with data bases in Redshift, Oracle, Trino, Snowflake, or other similar systems\n",
      "* You are a SQL expert, and have experience with multiple data visualization or business intelligence tools. Experience with Sigma Computing is preferred.\n",
      "* You have some experience in python or similar programming languages to build ETL pipelines.\n",
      "* Identifying how data can be structured and leveraged to drive meaningful insights.\n",
      "* You excel at defining meaningful metrics and KPIs, ensuring data is structured and accessible for ongoing reporting, dashboarding, and deeper analytics.\"\n",
      "* You want to own insights across team and technology stack, including supporting product development, operations, and commercialization.\n",
      "* You proactively engage with stakeholders to understand business challenges.\n",
      "\n",
      " Notice to Applicants for Jobs Located in NYC or Remote Jobs Associated With Office in NYC Only\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " We use Covey as part of our hiring and/or promotional process for jobs in NYC and certain features may qualify it as an AEDT in NYC. As part of the hiring and/or promotion process, we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound from August 21, 2023, through December 21, 2023, and resumed using Covey Scout for Inbound again on June 29, 2024\\.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " The Covey tool has been reviewed by an independent auditor. Results of the audit may be viewed here: Covey\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "About DoorDash\n",
      " At DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door\\-to\\-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " DoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We're committed to supporting employees' happiness, healthiness, and overall well\\-being by providing comprehensive benefits and perks.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "Our Commitment to Diversity and Inclusion\n",
      " We're committed to growing and empowering a more inclusive community within our company, industry, and cities. That's why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " If you need any accommodations, please inform your recruiting contact upon initial connection.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"..\\\\models\\\\resume_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = [ent.text for ent in docs.ents if ent.label_ == 'SKILLS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_skills = [ent.text.lower() for ent in docs.ents if ent.label_ == 'SKILLS' or ent.label_ == \"CERTIFICATION\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sql data analysis data visualization']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arshid T M\n",
      "DATA SCIENTIST\n",
      "♂¶ap-¶arker-altKannur,Kerala /envel⌢pearshidtm2001@gmail.com ♂phone-alt+91 8078196576 < / > Leetcode /linkedin-inArshid T M\n",
      "/githubArshidtm\n",
      "Profile Summary\n",
      "Data Science and Machine Learning enthusiast with expertise in building predictive models, deep learning,\n",
      "and NLP. Skilled in data preprocessing, feature engineering, and EDA. Proficient in Python, TensorFlow, and\n",
      "Scikit-Learn for model development and deployment. Passionate about leveraging AI to solve real-world problems\n",
      "and drive business growth.\n",
      "Skills\n",
      "◦Programming Language : Python\n",
      "◦Data Analysis & Visualization : Pandas, NumPy, Matplotlib, Seaborn, Tableau\n",
      "◦Database Management : PostgreSQL\n",
      "◦Machine Learning :\n",
      "– Regression : Linear Regression, Random Forest Regressor\n",
      "– Classification : KNN, Naive Bayes, SVM, Decision Trees, Random Forest Classifier\n",
      "– Ensemble Learning : Bagging, XgBoost\n",
      "– Clustering : K-Means, DBSCAN\n",
      "– Advanced Topics : Natural Language Processing (NLP), Deep Learning\n",
      "Projects\n",
      "Sentiment Analysis GitHub\n",
      "Developed a Machine learning -based sentiment analysis project that applies NLP techniques like tokenization,\n",
      "stemming, and vectorization (TF-IDF & Word2Vec). Trained using Random Forest and Multinomial Na ¨ıve\n",
      "Bayes , achieving 96% accuracy with Grid Search tuning. Developed as a web application using Flask for real-\n",
      "time predictions.\n",
      "◦NLP Preprocessing : Tokenization, stemming, vectorization (TF-IDF & Word2Vec)\n",
      "◦Machine Learning Models : Implemented Random Forest & Multinomial Na¨ ıve Bayes for text classifica-\n",
      "tion.\n",
      "◦Hyperparameter Tuning : Used Grid Search to optimize model performance.\n",
      "◦Deployment : Hosted as a Flask web application for real-time predictions.\n",
      "◦Tools Used : Python — Pandas — NumPy — Scikit-Learn — NLTK — Matplotlib — Seaborn — Flask\n",
      "Movie Recommendation System – Content-Based Filtering GitHub\n",
      "Developed a content-based movie recommendation system that suggests movies based on the director,\n",
      "actors, and genre. Applied text preprocessing using NLP techniques, vectorized movie data using Bag of Words\n",
      "(BoW) , and calculated similarity scores using cosine similarity for accurate recommendations. Developed as\n",
      "an interactive Streamlit web app for seamless user experience.\n",
      "◦NLP Preprocessing : Cleaned and processed movie metadata\n",
      "◦Vectorization : Used Bag of words (BoW) for the representation of characteristics.\n",
      "◦Similarity Computation : Implemented cosine similarity for movie recommendations.\n",
      "◦Deployment : Built and deployed using Streamlit for a user-friendly interface.\n",
      "◦Tools Used : Python — Pandas — NumPy — Scikit-Learn — NLTK — Streamlit\n",
      "Mini Project\n",
      "WhatsApp Chat Analyzer GitHub\n",
      "◦Developed a WhatsApp Chat Analyzer that extracts insights from exported .txt files, performing EDA\n",
      "to visualize message frequency, participant activity, and emoji usage. Applied stop-word removal for text\n",
      "preprocessing and deployed as a Streamlit Web app for real-time analysis.\n",
      "◦Extracts and analyzes WhatsApp chat data from text files.\n",
      "◦Identifies chat patterns, word frequency, and emoji usage.\n",
      "◦Applied text preprocessing and stopword removal for cleaner analysis.\n",
      "◦Built using Streamlit for an interactive user experience.\n",
      "◦Tools Used : Python — Pandas — Matplotlib — Seaborn — Streamlit\n",
      "HR Analytics Dashboard Tableau\n",
      "◦Developed an interactive HR Analytics Dashboard using Tableau to analyze key workforce metrics, including\n",
      "employee count, attrition rate, department-wise attrition, job satisfaction, and education-wise attrition.\n",
      "◦Visualizes attrition trends by department, education field, and gender for data-driven HR decision-making.\n",
      "◦Analyzes the age distribution of the employees and the satisfaction ratings in various job roles.\n",
      "◦Built using Tableau for an interactive and user-friendly experience.\n",
      "◦Tools Used : Tableau\n",
      "Email Spam Detection GitHub\n",
      "◦Developed an Email Spam Detection System using NLP and machine learning to classify emails as spam or\n",
      "ham.\n",
      "◦Applied text preprocessing techniques like tokenization, stopword removal, and stemming for data cleaning.\n",
      "◦Identified frequent words in spam and ham emails using WordCloud visualization.\n",
      "◦Used TF-IDF vectorization to convert text into numerical features.\n",
      "◦Trained and compared multiple models (GaussianNB, MultinomialNB, BernoulliNB), with TF-IDF + Multi-\n",
      "nomialNB achieving the best precision score.\n",
      "◦Streamlit Web App Deployed for Real-Time Email Classification.\n",
      "◦Tools Used :Python — Pandas — Scikit-learn — NLTK — Streamlit\n",
      "Education\n",
      "Data Science\n",
      "Brototype, Remote2024 - present\n",
      "Master of Science in Data Science and Business Analysis\n",
      "Rathinam College of Arts and Science - Bharatiyar University, Coimbatore2022 - 2024\n",
      "CGPA: 7.9\n",
      "Bachelor of Science in Mathematics\n",
      "Sir Syed College - Kannur University, Kannur2019 - 2022\n",
      "CGPA: 7.067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"..\\\\data\\\\Arshid.pdf\",'rb') as file:\n",
    "    pdf_reader = PdfReader(file)\n",
    "    full_text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:  # Only add if text exists\n",
    "            full_text += page_text + \"\\n\"  # Add newline between pages\n",
    "    \n",
    "    # Print the extracted text\n",
    "    print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_docs = nlp(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_skills = [ent.text.lower() for ent in full_text_docs.ents if ent.label_ == 'SKILLS' or ent.label_ == \"CERTIFICATION\"  or ent.label_ == \"DEGREE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandas',\n",
       " 'numpy',\n",
       " 'matplotlib',\n",
       " 'seaborn',\n",
       " 'sentiment analysis github',\n",
       " 'master of science in data science and business analysis',\n",
       " 'cgpa: 7.9',\n",
       " 'bachelor of science in mathematics\\nsir syed college - kannur university, kannur2019 - 2022',\n",
       " 'cgpa: 7.067']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_gap = [skill for skill in docs_skills if skill not in resume_skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning (ml)', 'fastapi', 'pytorch', 'opencv']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"..\\\\data\\\\combined_courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Cybersecurity</td>\n",
       "      <td>Google</td>\n",
       "      <td>network security, python programming, linux, ...</td>\n",
       "      <td>Coursera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Data Analytics</td>\n",
       "      <td>Google</td>\n",
       "      <td>data analysis, r programming, sql, business c...</td>\n",
       "      <td>Coursera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Project Management:</td>\n",
       "      <td>Google</td>\n",
       "      <td>project management, strategy and operations, ...</td>\n",
       "      <td>Coursera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBM Data Science</td>\n",
       "      <td>IBM</td>\n",
       "      <td>python programming, data science, machine lea...</td>\n",
       "      <td>Coursera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Digital Marketing &amp; E-commerce</td>\n",
       "      <td>Google</td>\n",
       "      <td>digital marketing, marketing, marketing manag...</td>\n",
       "      <td>Coursera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title Organization  \\\n",
       "0                   Google Cybersecurity       Google   \n",
       "1                  Google Data Analytics       Google   \n",
       "2             Google Project Management:       Google   \n",
       "3                       IBM Data Science          IBM   \n",
       "4  Google Digital Marketing & E-commerce       Google   \n",
       "\n",
       "                                              Skills  Platform  \n",
       "0   network security, python programming, linux, ...  Coursera  \n",
       "1   data analysis, r programming, sql, business c...  Coursera  \n",
       "2   project management, strategy and operations, ...  Coursera  \n",
       "3   python programming, data science, machine lea...  Coursera  \n",
       "4   digital marketing, marketing, marketing manag...  Coursera  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\models\\\\skill_embeddings.pkl\",'rb') as f:\n",
    "    skill_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_gap = \", \".join(skill_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning (ml), fastapi, pytorch, opencv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_gap_embedding = embeddings.embed_query(skill_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = [cosine_similarity([skill_gap_embedding], [embedding])[0][0] for embedding in skill_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_recommended = ' '\n",
    "for i in top_indices: \n",
    "    course_recommended += f\"{df.iloc[i]['Title']} - {df.iloc[i]['Organization']} - {df.iloc[i]['Platform']}, \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introduction to Computer Vision and Image Processing - IBM - Coursera, Computer Vision with Pytorch - Deep Learning 2023 - Updated - Udemy - Udemy, Deep Learning with Python & Pytorch for Image Classification - Udemy - Udemy, \n"
     ]
    }
   ],
   "source": [
    "print(course_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_course(vector_store, nlp, resume_skills, embeddings, skill_embeddings, df):\n",
    "    vec_data = vector_store.docstore._dict.values()\n",
    "    # Convert to list if needed\n",
    "    documents = list(vec_data)\n",
    "    texts = ''\n",
    "    for doc in documents:\n",
    "        texts += doc.page_content\n",
    "    \n",
    "    docs = nlp(texts)\n",
    "    docs_skills = [ent.text.lower() for ent in docs.ents if ent.label_ == 'SKILLS' or ent.label_ == \"CERTIFICATION\" ]\n",
    "    \n",
    "    skill_gap = \", \".join([skill for skill in docs_skills if skill not in resume_skills])\n",
    "    \n",
    "    skill_gap_embedding = embeddings.embed_query(skill_gap)\n",
    "    \n",
    "    similarity_scores = [cosine_similarity([skill_gap_embedding], [embedding])[0][0] for embedding in skill_embeddings]\n",
    "    \n",
    "    top_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:3]\n",
    "    \n",
    "    course_recommended = ' '\n",
    "    for i in top_indices: \n",
    "        course_recommended += f\"{df.iloc[i]['Title']} - {df.iloc[i]['Organization']} - {df.iloc[i]['Platform']}, \"\n",
    "    \n",
    "    return course_recommended\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_courses(vector_store, nlp, resume_skills, embeddings, skill_embeddings, df, top_n=3):\n",
    "    \"\"\"\n",
    "    Optimized course recommendation based on skill gaps with reduced time complexity.\n",
    "    \n",
    "    Args:\n",
    "        vector_store: FAISS vector store with job data\n",
    "        nlp: SpaCy NLP model\n",
    "        resume_skills: List of skills from resume (lowercase)\n",
    "        embeddings: Embedding model (same as used in vector store)\n",
    "        skill_embeddings: Precomputed embeddings for courses\n",
    "        df: DataFrame containing course information\n",
    "        top_n: Number of courses to recommend\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - skill_gaps: List of identified skill gaps\n",
    "        - recommended_courses: List of recommended courses\n",
    "        - similarity_scores: Confidence scores for recommendations\n",
    "    \"\"\"\n",
    "    # Convert resume skills to set for O(1) lookups\n",
    "    resume_skills_set = set(skill.lower() for skill in resume_skills)\n",
    "    \n",
    "    # Extract skills from vector store documents more efficiently\n",
    "    skill_gaps = set()\n",
    "    for doc in vector_store.docstore._dict.values():\n",
    "        doc_skills = [ent.text.lower() for ent in nlp(doc.page_content).ents \n",
    "                     if ent.label_ in ('SKILLS', 'CERTIFICATION')]\n",
    "        skill_gaps.update(skill for skill in doc_skills if skill not in resume_skills_set)\n",
    "    \n",
    "    if not skill_gaps:\n",
    "        return {\"skill_gaps\": [], \"recommended_courses\": [], \"similarity_scores\": []}\n",
    "    \n",
    "    # Convert skill_gaps to embedding in one batch\n",
    "    skill_gap_text = \" \".join(skill_gaps)\n",
    "    skill_gap_embedding = embeddings.embed_query(skill_gap_text)\n",
    "    \n",
    "    # Vectorized cosine similarity calculation\n",
    "    similarity_scores = cosine_similarity(\n",
    "        [skill_gap_embedding],\n",
    "        skill_embeddings\n",
    "    )[0]\n",
    "    \n",
    "    # Get top N courses using numpy for better performance\n",
    "    top_indices = np.argpartition(similarity_scores, -top_n)[-top_n:]\n",
    "    top_indices = top_indices[np.argsort(similarity_scores[top_indices])[::-1]]\n",
    "    \n",
    "    # Prepare results\n",
    "    recommended_courses = []\n",
    "    for idx in top_indices:\n",
    "        course = {\n",
    "            \"title\": df.iloc[idx]['Title'],\n",
    "            \"organization\": df.iloc[idx]['Organization'],\n",
    "            \"platform\": df.iloc[idx]['Platform'],\n",
    "            \n",
    "        }\n",
    "        recommended_courses.append(course)\n",
    "    \n",
    "    return recommended_courses\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_recommended = recommend_courses(vector_store,nlp,resume_skills,embeddings,skill_embeddings,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Computer Vision and Machine Learning with OpenCV 4',\n",
       "  'organization': 'Udemy',\n",
       "  'platform': 'Udemy'},\n",
       " {'title': 'Computer Vision with Pytorch - Deep Learning 2023 - Updated',\n",
       "  'organization': 'Udemy',\n",
       "  'platform': 'Udemy'},\n",
       " {'title': 'PyTorch: Deep Learning with PyTorch - Masterclass!: 2-in-1',\n",
       "  'organization': 'Udemy',\n",
       "  'platform': 'Udemy'}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\"\"\"\n",
    "You are a friendly and knowledgeable AI Career Mentor. Your role is to analyze skill gaps between a user's extracted resume skills and job descriptions from LinkedIn. You will provide insightful and practical recommendations to help users improve their qualifications and bridge any skill gaps. Be honest, supportive, and solution-oriented.\n",
    "\n",
    "Instructions:\n",
    "Respond to questions within the scope of the provided vector store, which includes job descriptions and skills for technical roles ,  If a question is outside this scope, please respond with 'No relevant information found in the vector store.'\"\n",
    "1. If the context provided is a casual greeting such as \"Hi\", \"Hello\", or \"How are you?\", respond briefly by introducing who you are. Avoid adding any unnecessary or unrelated information.\n",
    "2. Carefully analyze the context provided, which includes the user's extracted skills and job requirements from the vector store. Ensure you have a thorough understanding of the user's skills and the market-relevant skills retrieved from the vector store.\n",
    "3. Identify missing or underdeveloped skills and suggest actionable steps to bridge these gaps. \n",
    "   - Provide resources like courses, certifications, or projects that are directly relevant to the identified skill gaps.\n",
    "   - Recommend practical experience or networking opportunities where relevant and feasible.\n",
    "4. Offer clear, concise, and easy-to-understand responses tailored to the user's career growth. Avoid using jargon or technical terms that may confuse the user.\n",
    "5. \"Only generate responses based on context retrieved from the vector store. If no relevant context is found, respond with 'I don't know' or 'No information available'. Avoid generating speculative or generic responses. Provide accurate and relevant information only when it can be retrieved from the vector store.\"\n",
    "6. Provide alternative options where possible and avoid making up information or giving speculative responses. If you're unsure or lack sufficient information to provide an accurate answer, say so and ask for clarification or more context.\n",
    "7. Do not treat the vector store data as a specific job description — it is intended for understanding market-relevant skills. Keep your responses focused on the user's skills and career growth, rather than specific job openings.\n",
    "8. Always prioritize accuracy and transparency in your responses. If you're unable to provide a helpful answer, say so and encourage the user to seek additional resources or guidance.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_store.as_retriever(search_type='similarity',search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt  = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n",
    "    \"Formulate a standalone query which can be understood without the chat history.\"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return as it is.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        ('human',\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_filter(input_dict):\n",
    "    if not input_dict.get(\"context\"):\n",
    "        return {\"answer\": \"I can only answer questions about technical career skills based on my knowledge base.\"}\n",
    "    \n",
    "    # Additional check for document relevance\n",
    "    # if all(doc.metadata.get('relevance_score', 0) < 0.3 for doc in input_dict[\"context\"]):\n",
    "    #     return {\"answer\": \"I don't have enough relevant information about this specific technical career question.\"}\n",
    "    \n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "filtered_rag_chain = base_rag_chain | RunnableLambda(context_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    filtered_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input, session_id=\"default_session\"):\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = f\"Take  this as my  resume and suggest improvements:\\n{resume_skills} \\n plan a 6 month carrer plan using this course {course_recommended}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = get_response(user_input1,session_id=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a friendly AI Career Mentor. \n",
      "\n",
      "Based on your resume, I've identified some areas for improvement and skill gaps:\n",
      "\n",
      "1. **Limited programming skills**: Your resume only mentions Python libraries like pandas, numpy, matplotlib, and seaborn. Consider expanding your programming skills to other languages like R, SQL, or Java.\n",
      "2. **No experience with machine learning frameworks**: Although you have a Master's degree in Data Science and Business Analysis, your resume doesn't mention experience with machine learning frameworks like TensorFlow, Keras, or PyTorch. The courses you've listed will help bridge this gap.\n",
      "3. **No experience with computer vision**: The courses you've listed will also help you gain experience with computer vision, which is a valuable skill in the industry.\n",
      "4. **No projects or achievements mentioned**: Consider adding projects or achievements you've worked on, especially those related to sentiment analysis, to demonstrate your skills and experience.\n",
      "\n",
      "To create a 6-month career plan, I'll outline the following steps:\n",
      "\n",
      "**Month 1-2:**\n",
      "\n",
      "* Complete the course \"Computer Vision and Machine Learning with OpenCV 4\" on Udemy to gain a solid foundation in computer vision and machine learning.\n",
      "* Start working on projects that apply computer vision techniques, such as image classification, object detection, or segmentation.\n",
      "* Explore PyTorch and its applications in deep learning.\n",
      "\n",
      "**Month 3-4:**\n",
      "\n",
      "* Complete the course \"Computer Vision with Pytorch - Deep Learning 2023 - Updated\" on Udemy to deepen your understanding of PyTorch and computer vision.\n",
      "* Work on projects that involve PyTorch, such as building and deploying computer vision models.\n",
      "* Start learning about other machine learning frameworks like TensorFlow or Keras.\n",
      "\n",
      "**Month 5-6:**\n",
      "\n",
      "* Complete the course \"PyTorch: Deep Learning with PyTorch - Masterclass!: 2-in-1\" on Udemy to master PyTorch and deep learning concepts.\n",
      "* Develop a portfolio of projects that demonstrate your skills in computer vision, machine learning, and deep learning.\n",
      "* Prepare for interviews by practicing common data science and machine learning interview questions.\n",
      "\n",
      "Additional recommendations:\n",
      "\n",
      "* Join online communities like Kaggle, GitHub, or Reddit to stay updated on the latest developments in computer vision and machine learning.\n",
      "* Network with professionals in the industry to learn about new opportunities and best practices.\n",
      "* Consider participating in hackathons or competitions to demonstrate your skills and learn from others.\n",
      "\n",
      "By following this 6-month career plan, you'll be well on your way to bridging the skill gaps and becoming a more competitive candidate in the job market.\n"
     ]
    }
   ],
   "source": [
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id=\"abc123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input2 = \"can you give me how can i divide this 6 month career plan \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = get_response(user_input2,session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a more detailed breakdown of the 6-month career plan, divided into weekly and monthly tasks:\n",
      "\n",
      "**Month 1: Data Science Fundamentals and Python Review (Weeks 1-4)**\n",
      "\n",
      "* Week 1:\n",
      "\t+ Take online courses to learn data science basics (e.g., Data Science with Python, Data Science Essentials)\n",
      "\t+ Review Python basics (e.g., data types, functions, modules)\n",
      "* Week 2:\n",
      "\t+ Learn data preprocessing and visualization techniques (e.g., pandas, numpy, matplotlib)\n",
      "\t+ Practice data manipulation and visualization using sample datasets\n",
      "* Week 3:\n",
      "\t+ Study data modeling and machine learning basics (e.g., scikit-learn, TensorFlow)\n",
      "\t+ Work on simple machine learning projects (e.g., regression, classification)\n",
      "* Week 4:\n",
      "\t+ Review and practice data science concepts learned so far\n",
      "\t+ Start exploring more advanced data science topics (e.g., deep learning, natural language processing)\n",
      "\n",
      "**Month 2: PyTorch and Deep Learning (Weeks 5-8)**\n",
      "\n",
      "* Week 5:\n",
      "\t+ Enroll in the \"PyTorch: Deep Learning with PyTorch - Masterclass!: 2-in-1\" course on Udemy\n",
      "\t+ Learn PyTorch basics (e.g., tensors, autograd, modules)\n",
      "* Week 6:\n",
      "\t+ Study deep learning concepts (e.g., convolutional neural networks, recurrent neural networks)\n",
      "\t+ Practice building simple deep learning models using PyTorch\n",
      "* Week 7:\n",
      "\t+ Learn about PyTorch's advanced features (e.g., data loaders, batch normalization)\n",
      "\t+ Work on more complex deep learning projects (e.g., image classification, object detection)\n",
      "* Week 8:\n",
      "\t+ Review and practice PyTorch and deep learning concepts learned so far\n",
      "\t+ Explore more advanced PyTorch topics (e.g., transfer learning, attention mechanisms)\n",
      "\n",
      "**Month 3: OpenCV and Computer Vision (Weeks 9-12)**\n",
      "\n",
      "* Week 9:\n",
      "\t+ Take the \"Computer Vision and Machine Learning with OpenCV 4\" course on Udemy\n",
      "\t+ Learn OpenCV basics (e.g., image processing, feature detection)\n",
      "* Week 10:\n",
      "\t+ Study computer vision concepts (e.g., object recognition, tracking, segmentation)\n",
      "\t+ Practice building computer vision projects using OpenCV (e.g., image classification, object detection)\n",
      "* Week 11:\n",
      "\t+ Learn about OpenCV's advanced features (e.g., optical flow, stereo vision)\n",
      "\t+ Work on more complex computer vision projects (e.g., image segmentation, object recognition)\n",
      "* Week 12:\n",
      "\t+ Review and practice OpenCV and computer vision concepts learned so far\n",
      "\t+ Explore more advanced computer vision topics (e.g., deep learning-based computer vision)\n",
      "\n",
      "**Month 4: FastAPI and Web Development (Weeks 13-16)**\n",
      "\n",
      "* Week 13:\n",
      "\t+ Learn FastAPI basics (e.g., routing, requests, responses)\n",
      "\t+ Build simple web applications using FastAPI\n",
      "* Week 14:\n",
      "\t+ Study FastAPI's advanced features (e.g., middleware, dependencies)\n",
      "\t+ Practice building more complex web applications using FastAPI (e.g., RESTful APIs, web scraping)\n",
      "* Week 15:\n",
      "\t+ Learn about FastAPI's integration with other frameworks and libraries (e.g., PyTorch, OpenCV)\n",
      "\t+ Work on projects that integrate FastAPI with other technologies (e.g., machine learning, computer vision)\n",
      "* Week 16:\n",
      "\t+ Review and practice FastAPI and web development concepts learned so far\n",
      "\t+ Explore more advanced web development topics (e.g., microservices, containerization)\n",
      "\n",
      "**Month 5-6: Project Development and Practice (Weeks 17-24)**\n",
      "\n",
      "* Week 17-20:\n",
      "\t+ Work on building a comprehensive project that integrates data science, PyTorch, OpenCV, and FastAPI\n",
      "\t+ Practice and refine your skills by working on various projects and exercises\n",
      "* Week 21-22:\n",
      "\t+ Participate in Kaggle competitions or hackathons to apply your skills to real-world problems\n",
      "\t+ Learn from others and get feedback on your projects and code\n",
      "* Week 23-24:\n",
      "\t+ Review and practice all the concepts learned throughout the 6-month career plan\n",
      "\t+ Prepare for potential job interviews or further education by refining your portfolio and skills\n",
      "\n",
      "Remember, this is just a suggested plan, and you should adjust it according to your needs, schedule, and learning pace. Stay motivated, and don't hesitate to reach out if you have any questions or need further guidance!\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"doc_pickle_sample.pkl\",\"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(docs):\n",
    "    text = \"\"\n",
    "    \n",
    "    for i,doc in enumerate(docs):\n",
    "        text += doc.page_content\n",
    "        if i == 3:\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = doc_to_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Role Description\\n We are seeking a skilled\\n Data Analyst \\n with 4\\\\-6 years of experience to join our team. The ideal candidate will have strong expertise in Data analysis,\\n SQL \\n , Data Visualization coupled with excellent analytical and data validation skills. A solid grasp of statistical techniques and the ability to extract actionable insights from complex datasets is essential.\\n   \\n\\n  \\n\\nLocation \\\\- Pune or Kolkata\\nKey Responsibilities \\n  \\n\\n  \\n\\n* Data Pipeline Development:\\n+ Design and implement robust data pipelines for data ingestion, cleansing, and transformation.\\n\\n* Data Extraction and Reporting:\\n+ Extract relevant information from multiple data sources and create detailed reports to support stakeholder decision\\\\-making.\\n\\n* Cross\\\\-Functional Collaboration:\\n+ Collaborate with cross\\\\-functional teams to understand business goals and translate them into analytical solutions.\\n\\n* Statistical Analysis:\\n+ Utilize statistical techniques to analyze data, identifying patterns and trends for strategic decision\\\\-making.\\n\\n* Exploratory Data Analysis (EDA):\\n+ Perform exploratory data analysis to uncover insights and generate hypotheses for further investigation.\\n\\n* Data Visualization:\\n+ Develop and present clear, impactful visualizations and reports to communicate findings and recommendations effectively.\\n\\n* Management Reporting:\\n+ Create comprehensive management reports that highlight trends, patterns, and actionable predictions based on data.\\n\\nPrimary Skills\\n* Good knowledge in Python or PySpark.\\n* Strong SQL skills for data analysis and troubleshooting.\\n* Advanced data analysis and validation expertise.\\n* Proficiency in creating clear and actionable data visualizations.\\n\\nSecondary Skills (Good To Have)\\n* Experience with MS Excel.\\n* Familiarity with Databricks for advanced data processing and collaboration.\\n\\nQualifications\\n* 4\\\\-6 years of experience in data analysis or a related field.\\n* Strong problem\\\\-solving and troubleshooting skills using SQL.\\n\\nSkills\\n SQL Data Analysis Data Visualization, Python or PySparkJob Description\\n Be part of the solution at Technip Energies and embark on a one\\\\-of\\\\-a\\\\-kind journey. You will be helping to develop cutting\\\\-edge solutions to solve real\\\\-world energy problems.\\n   \\n\\n  \\n\\n We are currently seeking a Climate Data Analyst, reporting directly to\\n *\\\\[reporting line position]* \\n to join our team based in Noida.\\n   \\n\\n  \\n\\n The Climate Data Analyst will be responsible for collecting, analyzing, and reporting data related to the company’s Greenhouse Gas emissions (scope 3\\\\). They will play a crucial role in supporting the development and implementation of Climate Transition strategies and action plans by providing insights derived from data analysis.\\n   \\n\\n  \\n\\nAbout us:\\n Technip Energies is a global technology and engineering powerhouse. With leadership positions in LNG, hydrogen, ethylene, sustainable chemistry, and CO2 management, we are contributing to the development of critical markets such as energy, energy derivatives, decarbonization, and circularity. Our complementary business segments, Technology, Products and Services (TPS) and Project Delivery, turn innovation into scalable and industrial reality.\\n   \\n\\n  \\n\\n Through collaboration and excellence in execution, our 17,000\\\\+ employees across 34 countries are fully committed to bridging prosperity with sustainability for a world designed to last.\\n   \\n\\n  \\n\\nAbout the opportunity we offer:\\n The Climate Data Analyst will be responsible for collecting, analyzing, and reporting data related to the company’s Greenhouse Gas emissions (scope 3\\\\). They will play a crucial role in supporting the development and implementation of Climate Transition strategies and action plans by providing insights derived from data analysis.\\n   \\n\\n  \\n\\nJob Accountabilities:\\n* Carbon calculation (for scope 3 emissions)\\n* Maintain and improve Carbon calculation tools\\n* Ensure data accuracy, consistency, and integrity through regular audits as part of Data Quality run process.\\n* Train and support projects team in calculating the carbon footprint on projects\\n* Maintain Emission factors to be up\\\\-to\\\\-date\\n* Ensure proper collection of the data from carbon calculation\\n* Analyze data to identify trends, patterns, and insights related to GHG emissions reductions.\\n* Prepare reports, dashboards and presentations to communicate GHG emissions performance.\\n* Climate scenario analysis\\n* Improve the Climate scenario analysis tool\\n* Participate in elaborating different scenarios of GHG emissions reduction\\n* Collaborate with finance modeling to ensure the alignment of data forecast\\n* Elaborate a tool to monitor GHG emissions reduction\\n* Data Analysis\\n* Data crunching and competitive analysis for the various Climate\\\\-related topics.\\n* Collect and manage climate\\\\-related data from various internal and external sources.\\n* Support the Climate change team to optimize internal processes by using digital tools and artificial intelligence.\\n* Gather data and develop a strategic competitor analysis dashboard and reports.\\n* Collaborate with cross\\\\-functional teams, especially IDS and Digital to identify opportunities and develop solutions for the sustainability department.\\n* Contribute to the continuous improvement of sustainability data management processes and systems.\\n\\nAbout you:\\n Bachelor degree or superior in engineering and/or data sciences\\n   \\n\\n  \\n\\n* 4\\\\+ years of relevant work experience\\n* Mastery of Microsoft Tools (PowerPoint, Word, Excel)\\n* Hands\\\\-on experience with data visualization tools (PowerBI preferably)\\n* Knowledge of Python programming and databases\\n* Knowledge of artificial intelligence and machine learning\\n* Excellent data analysis and organizational skills\\n* Strong analytical and data management skills\\n* Ability to work independently and manage multiple projects simultaneously\\n* Knowledge of sustainability issues and commitment to driving sustainable change is a differentiator\\n* Comfortable with complex ecosystem, moving environment and multi\\\\-stakeholders’ management\\n* Good communication and interpersonal skills to effectively collaborate with various teams\\n* Fluent English level is required with good writing skills\\n\\nWhat’s next?\\n Once receiving your application, our Talent Acquisition professionals will screen and match your profile against the role requirements. We ask for your patience as the team completes the volume of applications with reasonable timeframe. Check your application progress periodically via personal account from created candidate profile during your application.\\n   \\n\\n  \\n\\n We invite you to get to know more about our company by visiting and follow us on LinkedIn, Instagram, Facebook, X and YouTube for company updates.Hyderabad, preferred or remote\\nAbout the Team\\n DoorDash Labs is an independent team within DoorDash. We explore robotics and automation to transform last mile logistics in the long term. If you want to work on commercializing autonomy and robotics in a service used by millions of people, then we want to talk to you!\\n   \\n\\n  \\n\\nAbout the Role\\n Come help us redefine last\\\\-mile logistics through robotics, automation, and other advanced technologies! DoorDash Labs is an independent team inside of DoorDash incubating long\\\\-term bets and exploring new technologies that can be applied to transform last\\\\-mile logistics. We believe in the value of advanced technologies and strive to build meaningful applications of next\\\\-generation hardware and software to impact real\\\\-world needs.\\n   \\n\\n  \\n\\n We\\'re looking for a data analyst to help bring autonomous robots and other high\\\\-tech automation products to market, at scale.\\n   \\n\\n  \\n\\nYou\\'re excited about this opportunity because you will…\\n* Own analytics and insights across multiple technical and product areas, ensuring data is structured to support clear measurement of business and product performance.\\n* Define a strategy for commercial, operations, and autonomy insights to drive product improvements and answer critical business questions.\\n* Report in directly with the product management team, and support a world\\\\-class team of engineers and business leaders to bring autonomous solutions to market\\n* Use your skill in understanding and synthesis of data to create high value visualization and dashboards\\n* Act as the primary data analytics partner for stakeholders across engineering, product, and leadership, helping to translate complex business questions into measurable data solutions.\\n\\nWe\\'re excited about you because…\\n* You have a Bachelor\\'s degree in business administration, economics, computer science, management information systems, or related field or equivalent related experience\\n* You have 3\\\\-6 years of experience in data analytics or business analytics\\n* You have experience with data bases in Redshift, Oracle, Trino, Snowflake, or other similar systems\\n* You are a SQL expert, and have experience with multiple data visualization or business intelligence tools. Experience with Sigma Computing is preferred.\\n* You have some experience in python or similar programming languages to build ETL pipelines.\\n* Identifying how data can be structured and leveraged to drive meaningful insights.\\n* You excel at defining meaningful metrics and KPIs, ensuring data is structured and accessible for ongoing reporting, dashboarding, and deeper analytics.\"\\n* You want to own insights across team and technology stack, including supporting product development, operations, and commercialization.\\n* You proactively engage with stakeholders to understand business challenges.\\n\\n Notice to Applicants for Jobs Located in NYC or Remote Jobs Associated With Office in NYC Only\\n   \\n\\n  \\n\\n We use Covey as part of our hiring and/or promotional process for jobs in NYC and certain features may qualify it as an AEDT in NYC. As part of the hiring and/or promotion process, we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound from August 21, 2023, through December 21, 2023, and resumed using Covey Scout for Inbound again on June 29, 2024\\\\.\\n   \\n\\n  \\n\\n The Covey tool has been reviewed by an independent auditor. Results of the audit may be viewed here: Covey\\n   \\n\\n  \\n\\nAbout DoorDash\\n At DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door\\\\-to\\\\-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods.\\n   \\n\\n  \\n\\n DoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We\\'re committed to supporting employees\\' happiness, healthiness, and overall well\\\\-being by providing comprehensive benefits and perks.\\n   \\n\\n  \\n\\nOur Commitment to Diversity and Inclusion\\n We\\'re committed to growing and empowering a more inclusive community within our company, industry, and cities. That\\'s why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel.\\n   \\n\\n  \\n\\n If you need any accommodations, please inform your recruiting contact upon initial connection.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    max_tokens = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an intelligent text skill parsing assistant. Your task is to extract only the skills mentioned in the given text and return them in a clean, valid JSON format.\n",
    "\n",
    "**Text:**\n",
    "{text}\n",
    "\n",
    "**Instructions:**\n",
    "- Identify all skills present in the text, including programming languages, tools, frameworks, and technologies.\n",
    "- Return the skills as a JSON list.\n",
    "- Do not include any extra explanation, formatting, or comments.\n",
    "- if the extracted skill is FUll EXPANDED form or any synonyms give them in its root form. \n",
    "- The resulted skills should be in its root form\n",
    "- Only return the JSON list of skills.\n",
    "\n",
    "**Example Output Format:**\n",
    "[\"Python\", \"TensorFlow\", \"Pandas\", \"Tableau\"]\n",
    "\n",
    "strictly return in json dictionary format.\n",
    "Always return in same format\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_skill(text,llm,prompt_template): \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    formatted_prompt = prompt.format(text=text)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    parsed_data = response.content\n",
    "    \n",
    "    cleaned = re.sub(r\"^```json|```$\", \"\", parsed_data.strip(), flags=re.MULTILINE)\n",
    "    dict_parsed_data = json.loads(cleaned)\n",
    "    \n",
    "    # skill_dics = dict_parsed_data['skills']\n",
    "    \n",
    "    return dict_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:34:03,139 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "skills = parse_skill(text,llm,prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arshid T M\\nDATA SCIENTIST\\n♂¶ap-¶arker-altKannur,Kerala /envel⌢pearshidtm2001@gmail.com ♂phone-alt+91 8078196576 < / > Leetcode /linkedin-inArshid T M\\n/githubArshidtm\\nProfile Summary\\nData Science and Machine Learning enthusiast with expertise in building predictive models, deep learning,\\nand NLP. Skilled in data preprocessing, feature engineering, and EDA. Proficient in Python, TensorFlow, and\\nScikit-Learn for model development and deployment. Passionate about leveraging AI to solve real-world problems\\nand drive business growth.\\nSkills\\n◦Programming Language : Python\\n◦Data Analysis & Visualization : Pandas, NumPy, Matplotlib, Seaborn, Tableau\\n◦Database Management : PostgreSQL\\n◦Machine Learning :\\n– Regression : Linear Regression, Random Forest Regressor\\n– Classification : KNN, Naive Bayes, SVM, Decision Trees, Random Forest Classifier\\n– Ensemble Learning : Bagging, XgBoost\\n– Clustering : K-Means, DBSCAN\\n– Advanced Topics : Natural Language Processing (NLP), Deep Learning\\nProjects\\nSentiment Analysis GitHub\\nDeveloped a Machine learning -based sentiment analysis project that applies NLP techniques like tokenization,\\nstemming, and vectorization (TF-IDF & Word2Vec). Trained using Random Forest and Multinomial Na ¨ıve\\nBayes , achieving 96% accuracy with Grid Search tuning. Developed as a web application using Flask for real-\\ntime predictions.\\n◦NLP Preprocessing : Tokenization, stemming, vectorization (TF-IDF & Word2Vec)\\n◦Machine Learning Models : Implemented Random Forest & Multinomial Na¨ ıve Bayes for text classifica-\\ntion.\\n◦Hyperparameter Tuning : Used Grid Search to optimize model performance.\\n◦Deployment : Hosted as a Flask web application for real-time predictions.\\n◦Tools Used : Python — Pandas — NumPy — Scikit-Learn — NLTK — Matplotlib — Seaborn — Flask\\nMovie Recommendation System – Content-Based Filtering GitHub\\nDeveloped a content-based movie recommendation system that suggests movies based on the director,\\nactors, and genre. Applied text preprocessing using NLP techniques, vectorized movie data using Bag of Words\\n(BoW) , and calculated similarity scores using cosine similarity for accurate recommendations. Developed as\\nan interactive Streamlit web app for seamless user experience.\\n◦NLP Preprocessing : Cleaned and processed movie metadata\\n◦Vectorization : Used Bag of words (BoW) for the representation of characteristics.\\n◦Similarity Computation : Implemented cosine similarity for movie recommendations.\\n◦Deployment : Built and deployed using Streamlit for a user-friendly interface.\\n◦Tools Used : Python — Pandas — NumPy — Scikit-Learn — NLTK — Streamlit\\nMini Project\\nWhatsApp Chat Analyzer GitHub\\n◦Developed a WhatsApp Chat Analyzer that extracts insights from exported .txt files, performing EDA\\nto visualize message frequency, participant activity, and emoji usage. Applied stop-word removal for text\\npreprocessing and deployed as a Streamlit Web app for real-time analysis.\\n◦Extracts and analyzes WhatsApp chat data from text files.\\n◦Identifies chat patterns, word frequency, and emoji usage.\\n◦Applied text preprocessing and stopword removal for cleaner analysis.\\n◦Built using Streamlit for an interactive user experience.\\n◦Tools Used : Python — Pandas — Matplotlib — Seaborn — Streamlit\\nHR Analytics Dashboard Tableau\\n◦Developed an interactive HR Analytics Dashboard using Tableau to analyze key workforce metrics, including\\nemployee count, attrition rate, department-wise attrition, job satisfaction, and education-wise attrition.\\n◦Visualizes attrition trends by department, education field, and gender for data-driven HR decision-making.\\n◦Analyzes the age distribution of the employees and the satisfaction ratings in various job roles.\\n◦Built using Tableau for an interactive and user-friendly experience.\\n◦Tools Used : Tableau\\nEmail Spam Detection GitHub\\n◦Developed an Email Spam Detection System using NLP and machine learning to classify emails as spam or\\nham.\\n◦Applied text preprocessing techniques like tokenization, stopword removal, and stemming for data cleaning.\\n◦Identified frequent words in spam and ham emails using WordCloud visualization.\\n◦Used TF-IDF vectorization to convert text into numerical features.\\n◦Trained and compared multiple models (GaussianNB, MultinomialNB, BernoulliNB), with TF-IDF + Multi-\\nnomialNB achieving the best precision score.\\n◦Streamlit Web App Deployed for Real-Time Email Classification.\\n◦Tools Used :Python — Pandas — Scikit-learn — NLTK — Streamlit\\nEducation\\nData Science\\nBrototype, Remote2024 - present\\nMaster of Science in Data Science and Business Analysis\\nRathinam College of Arts and Science - Bharatiyar University, Coimbatore2022 - 2024\\nCGPA: 7.9\\nBachelor of Science in Mathematics\\nSir Syed College - Kannur University, Kannur2019 - 2022\\nCGPA: 7.067\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:33:21,838 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "resume_skills = parse_skill(full_text,llm,prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_courses_llm(jd_skills,resume_skills, embeddings, skill_embeddings, df, top_n=3):\n",
    "    \"\"\"\n",
    "    Optimized course recommendation based on skill gaps with reduced time complexity.\n",
    "    \n",
    "    Args:\n",
    "           \n",
    "        jd_skills: List of skills from job_data         \n",
    "        resume_skills: List of skills from resume (lowercase)\n",
    "        embeddings: Embedding model (same as used in vector store) HuggingFaceEmbeddings\n",
    "        skill_embeddings: Precomputed embeddings for skills\n",
    "        df: DataFrame containing course information\n",
    "        top_n: Number of courses to recommend\n",
    "    \n",
    "    Returns:        \n",
    "        - recommended_courses: List of recommended courses       \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert resume skills to set for O(1) lookups\n",
    "        resume_skills_set = set(skill.lower() for skill in resume_skills)        \n",
    "\n",
    "        # Extract skills from vector store\n",
    "        jd_skills_set = set(skill.lower() for skill in jd_skills)\n",
    "        \n",
    "        skill_gaps = jd_skills_set - resume_skills_set      \n",
    "                \n",
    "        logger.info(f\"Identified skill gaps: {skill_gaps}\")\n",
    "        \n",
    "        if not skill_gaps:\n",
    "            logger.warning(\"No skill gaps found. Returning empty result.\")\n",
    "            return \"No skill gap found, Recommend some course to enhance skill\"\n",
    "\n",
    "        # Embed the skill gap text\n",
    "        skill_gap_text = \" \".join(skill_gaps)\n",
    "        skill_gap_embedding = embeddings.embed_query(skill_gap_text)\n",
    "        logger.info(\"Skill gap embedding generated.\")\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity_scores = cosine_similarity(\n",
    "            [skill_gap_embedding],\n",
    "            skill_embeddings\n",
    "        )[0]\n",
    "\n",
    "        # Get top N course indices\n",
    "        top_indices = np.argpartition(similarity_scores, -top_n)[-top_n:]\n",
    "        top_indices = top_indices[np.argsort(similarity_scores[top_indices])[::-1]]\n",
    "\n",
    "        # Prepare course recommendation results\n",
    "        recommended_courses = []\n",
    "        for idx in top_indices:\n",
    "            course = {\n",
    "                \"title\": df.iloc[idx].get('Title', 'N/A'),\n",
    "                \"organization\": df.iloc[idx].get('Organization', 'N/A'),\n",
    "                \"platform\": df.iloc[idx].get('Platform', 'N/A')                \n",
    "            }\n",
    "            recommended_courses.append(course)\n",
    "\n",
    "        logger.info(\"Top recommended courses generated.\")\n",
    "\n",
    "        return recommended_courses            \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in recommend_courses: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:34:49,908 - __main__ - INFO - Identified skill gaps: {'databricks', 'greenhouse gas emissions', 'powerbi', 'business administration', 'data analysis', 'climate scenario analysis', 'artificial intelligence', 'sigma computing', 'database', 'digital tools', 'snowflake', 'word', 'trino', 'business intelligence tools', 'microsoft tools', 'data visualization', 'data visualization tools', 'data quality', 'carbon calculation', 'sql', 'excel', 'climate data analysis', 'redshift', 'powerpoint', 'oracle', 'emission factors', 'computer science', 'management information systems', 'economics', 'etl pipelines', 'pyspark', 'ghg emissions reduction'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:34:51,653 - __main__ - INFO - Skill gap embedding generated.\n",
      "2025-04-14 15:34:54,412 - __main__ - INFO - Top recommended courses generated.\n"
     ]
    }
   ],
   "source": [
    "recommended_courses = recommend_courses_llm(skills, resume_skills, embeddings, skill_embeddings, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'IBM Data Analyst', 'organization': 'IBM', 'platform': 'Coursera'},\n",
       " {'title': 'IBM Data Analytics with Excel and R',\n",
       "  'organization': 'IBM',\n",
       "  'platform': 'Coursera'},\n",
       " {'title': 'Digital Transformation Using AI/ML with Google Cloud',\n",
       "  'organization': 'Google Cloud',\n",
       "  'platform': 'Coursera'}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = f\"Take  this as my  resume and suggest improvements:\\n{resume_skills} \\n plan a 6 month carrer plan using this course {recommended_courses}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 16:05:54,050 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = get_response(user_input1,session_id=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your resume, I've identified some strengths and areas for improvement:\n",
      "\n",
      "Strengths:\n",
      "- You have a strong foundation in Python and its libraries, including NumPy, Pandas, and Scikit-Learn.\n",
      "- You have experience with various machine learning algorithms, including supervised and unsupervised learning techniques.\n",
      "- You have familiarity with data visualization tools like Matplotlib, Seaborn, and Tableau.\n",
      "- You have knowledge of deep learning frameworks like TensorFlow.\n",
      "\n",
      "Areas for improvement:\n",
      "- You lack experience with large language models, retrieval-augmented generation frameworks, and computer vision techniques, which are in high demand in the industry.\n",
      "- You could improve your skills in building and deploying scalable REST APIs using frameworks like Flask or FastAPI.\n",
      "- You may want to consider learning more about cloud services like Azure or Google Cloud, as they are widely used in the industry.\n",
      "\n",
      "Considering your interests and skills, I've planned a 6-month career plan for you:\n",
      "\n",
      "**Month 1-2:**\n",
      "- Take the \"Digital Transformation Using AI/ML with Google Cloud\" course on Coursera to learn about AI/ML and cloud services.\n",
      "- Start building projects that integrate machine learning models with cloud services, such as deploying a model on Google Cloud or using cloud-based data storage.\n",
      "\n",
      "**Month 3-4:**\n",
      "- Take the \"IBM Data Analyst\" course on Coursera to improve your data analysis skills and learn about data visualization techniques.\n",
      "- Work on projects that involve data analysis, visualization, and machine learning, such as analyzing a dataset and building a predictive model.\n",
      "\n",
      "**Month 5-6:**\n",
      "- Take online courses or attend webinars to learn about large language models, retrieval-augmented generation frameworks, and computer vision techniques.\n",
      "- Build projects that demonstrate your skills in these areas, such as building a chatbot using a large language model or developing a computer vision application.\n",
      "\n",
      "Additionally, consider the following recommendations:\n",
      "- Join online communities like Kaggle, GitHub, or Reddit to network with other professionals and learn about new developments in the field.\n",
      "- Participate in hackathons or competitions to practice your skills and build your portfolio.\n",
      "- Read books or research papers to stay updated on the latest advancements in AI/ML and data science.\n",
      "\n",
      "By following this plan, you'll be able to enhance your skills, build a strong portfolio, and increase your chances of landing a job in the AI/ML industry.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
