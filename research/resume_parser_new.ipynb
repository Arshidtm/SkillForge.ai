{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6762d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ef892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee5447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1460075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3d2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c678f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(file_path): \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    extracted_text = \"\\n\".join(doc.page_content for doc in documents)\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d85df238",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    max_tokens = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e1f7565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        You are tasked with parsing a resume. Your objective is to extract relevant information in a valid structured 'JSON' format. Do not write explanations or any other preambles, do not add or write anything out of context, use only the given information.\n",
    "\n",
    "        **Task:** Extract key information from the following resume text, especially Skills, Experience, Education.\n",
    "\n",
    "        **Resume Text:**\n",
    "        {text}\n",
    "\n",
    "        **Instructions:**\n",
    "        Please extract the following information and format it in a clear structure as below. Assure to maintain these fields intact (do not change the case to lower):\n",
    "\n",
    "        1. **Contact Information:**\n",
    "        - Name:        \n",
    "\n",
    "        2. **Education:**\n",
    "        - Institution Name:\n",
    "        - Degree:\n",
    "        - Field of Study:\n",
    "        - Graduation Date:\n",
    "\n",
    "        3. **Experience:**\n",
    "        - Job Title:        \n",
    "        - Responsibilities/Projects:\n",
    "\n",
    "        4. **Projects:**\n",
    "        - Project Title:\n",
    "        - Description/Technologies Used:\n",
    "        - Outcomes/Results:\n",
    "\n",
    "        5. **Skills:**\n",
    "        - Programming Languages:\n",
    "        - Technologies/Tools/frameworks:\n",
    "\n",
    "        6. **Additional Information:** (if applicable)\n",
    "        - Certifications:\n",
    "        - Awards or Honors:\n",
    "        - Professional Affiliations:       \n",
    "        \n",
    "        strictly return in json dictionary format \n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an intelligent text skill parsing assistant. Your task is to extract only the skills mentioned in the given text and return them in a clean, valid JSON format.\n",
    "\n",
    "**Text:**\n",
    "{text}\n",
    "\n",
    "**Instructions:**\n",
    "- Identify all skills present in the text, including programming languages, tools, frameworks, and technologies.\n",
    "- Return the skills as a JSON list.\n",
    "- Do not include any extra explanation, formatting, or comments.\n",
    "- Only return the JSON list of skills.\n",
    "\n",
    "**Example Output Format:**\n",
    "[\"Python\", \"TensorFlow\", \"Pandas\", \"Tableau\"]\n",
    "\n",
    "strictly return in json dictionary format.\n",
    "Always return in same format\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c814ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5b15030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsed_resume_skils(text, prompt_template, llm, ):\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    \n",
    "    formatted_prompt = prompt.format(text=text)\n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    parsed_data = response.content\n",
    "    \n",
    "    cleaned = re.sub(r\"^```json|```$\", \"\", parsed_data.strip(), flags=re.MULTILINE)\n",
    "    dict_parsed_data = json.loads(cleaned)\n",
    "    \n",
    "    skill_dics = dict_parsed_data['skills']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return skill_dics\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "264f4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "155eaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_resume(text):\n",
    "    formatted_prompt = prompt.format(text=text)\n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5771be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9f57b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\arshi\\\\Downloads\\\\Desktop\\\\Bro-Project\\\\SkillForge.ai\\\\research'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5bf5353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_pdf_text(\"..\\\\data\\\\Arshid.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e08c1076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arshid T M\n",
      "DATA SCIENTIST\n",
      "♂¶ap-¶arker-altKannur,Kerala /envel⌢pearshidtm2001@gmail.com ♂phone-alt+91 8078196576 < / >Leetcode /linkedin-inArshid T M\n",
      "/githubArshidtm\n",
      "Profile Summary\n",
      "Data Science and Machine Learning enthusiastwith expertise in building predictive models, deep learning,\n",
      "and NLP. Skilled in data preprocessing, feature engineering, and EDA. Proficient in Python, TensorFlow, and\n",
      "Scikit-Learn for model development and deployment. Passionate about leveraging AI to solve real-world problems\n",
      "and drive business growth.\n",
      "Skills\n",
      "◦ Programming Language: Python\n",
      "◦ Data Analysis & Visualization : Pandas, NumPy, Matplotlib, Seaborn, Tableau\n",
      "◦ Database Management: PostgreSQL\n",
      "◦ Machine Learning:\n",
      "– Regression : Linear Regression, Random Forest Regressor\n",
      "– Classification : KNN, Naive Bayes, SVM, Decision Trees, Random Forest Classifier\n",
      "– Ensemble Learning : Bagging, XgBoost\n",
      "– Clustering : K-Means, DBSCAN\n",
      "– Advanced Topics: Natural Language Processing (NLP), Deep Learning\n",
      "Projects\n",
      "Sentiment Analysis GitHub\n",
      "Developed a Machine learning-based sentiment analysis project that applies NLP techniques like tokenization,\n",
      "stemming, and vectorization (TF-IDF & Word2Vec). Trained usingRandom Forest and Multinomial Na¨ıve\n",
      "Bayes, achieving 96% accuracy with Grid Search tuning. Developed as a web application using Flask for real-\n",
      "time predictions.\n",
      "◦ NLP Preprocessing: Tokenization, stemming, vectorization (TF-IDF & Word2Vec)\n",
      "◦ Machine Learning Models: Implemented Random Forest & Multinomial Na¨ ıve Bayes for text classifica-\n",
      "tion.\n",
      "◦ Hyperparameter Tuning: Used Grid Search to optimize model performance.\n",
      "◦ Deployment: Hosted as a Flask web application for real-time predictions.\n",
      "◦ Tools Used: Python — Pandas — NumPy — Scikit-Learn — NLTK — Matplotlib — Seaborn — Flask\n",
      "Movie Recommendation System – Content-Based Filtering GitHub\n",
      "Developed a content-based movie recommendation system that suggests movies based on the director,\n",
      "actors, and genre. Applied text preprocessing using NLP techniques, vectorized movie data usingBag of Words\n",
      "(BoW), and calculated similarity scores using cosine similarity for accurate recommendations. Developed as\n",
      "an interactive Streamlit web app for seamless user experience.\n",
      "◦ NLP Preprocessing: Cleaned and processed movie metadata\n",
      "◦ Vectorization: Used Bag of words (BoW) for the representation of characteristics.\n",
      "◦ Similarity Computation: Implemented cosine similarity for movie recommendations.\n",
      "◦ Deployment: Built and deployed using Streamlit for a user-friendly interface.\n",
      "◦ Tools Used: Python — Pandas — NumPy — Scikit-Learn — NLTK — Streamlit\n",
      "Mini Project\n",
      "WhatsApp Chat Analyzer GitHub\n",
      "◦ Developed a WhatsApp Chat Analyzer that extracts insights from exported .txt files, performing EDA\n",
      "to visualize message frequency, participant activity, and emoji usage. Applied stop-word removal for text\n",
      "preprocessing and deployed as a Streamlit Web app for real-time analysis.\n",
      "◦ Extracts and analyzes WhatsApp chat data from text files.\n",
      "◦ Identifies chat patterns, word frequency, and emoji usage.\n",
      "◦ Applied text preprocessing and stopword removal for cleaner analysis.\n",
      "◦ Built using Streamlit for an interactive user experience.\n",
      "◦ Tools Used: Python — Pandas — Matplotlib — Seaborn — Streamlit\n",
      "HR Analytics Dashboard Tableau\n",
      "◦ Developed an interactive HR Analytics Dashboard using Tableau to analyze key workforce metrics, including\n",
      "employee count, attrition rate, department-wise attrition, job satisfaction, and education-wise attrition.\n",
      "◦ Visualizes attrition trends by department, education field, and gender for data-driven HR decision-making.\n",
      "◦ Analyzes the age distribution of the employees and the satisfaction ratings in various job roles.\n",
      "◦ Built using Tableau for an interactive and user-friendly experience.\n",
      "◦ Tools Used: Tableau\n",
      "Email Spam Detection GitHub\n",
      "◦ Developed an Email Spam Detection System using NLP and machine learning to classify emails as spam or\n",
      "ham.\n",
      "◦ Applied text preprocessing techniques like tokenization, stopword removal, and stemming for data cleaning.\n",
      "◦ Identified frequent words in spam and ham emails using WordCloud visualization.\n",
      "◦ Used TF-IDF vectorization to convert text into numerical features.\n",
      "◦ Trained and compared multiple models (GaussianNB, MultinomialNB, BernoulliNB), with TF-IDF + Multi-\n",
      "nomialNB achieving the best precision score.\n",
      "◦ Streamlit Web App Deployed for Real-Time Email Classification.\n",
      "◦ Tools Used:Python — Pandas — Scikit-learn — NLTK — Streamlit\n",
      "Education\n",
      "Data Science\n",
      "Brototype, Remote\n",
      "2024 - present\n",
      "Master of Science in Data Science and Business Analysis\n",
      "Rathinam College of Arts and Science - Bharatiyar University, Coimbatore\n",
      "2022 - 2024\n",
      "CGPA: 7.9\n",
      "Bachelor of Science in Mathematics\n",
      "Sir Syed College - Kannur University, Kannur\n",
      "2019 - 2022\n",
      "CGPA: 7.067\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5fc0b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "persed_resume = parse_resume(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "56c1e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"skills\": [\"Python\", \"TensorFlow\", \"Scikit-Learn\", \"Pandas\", \"NumPy\", \"Matplotlib\", \"Seaborn\", \"Tableau\", \"PostgreSQL\", \"Linear Regression\", \"Random Forest Regressor\", \"KNN\", \"Naive Bayes\", \"SVM\", \"Decision Trees\", \"Random Forest Classifier\", \"Bagging\", \"XgBoost\", \"K-Means\", \"DBSCAN\", \"Natural Language Processing\", \"Deep Learning\", \"Flask\", \"NLTK\", \"Streamlit\", \"Word2Vec\", \"TF-IDF\", \"Grid Search\", \"Cosine Similarity\", \"Bag of Words\"]}\n"
     ]
    }
   ],
   "source": [
    "print(persed_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7407c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afff006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f66f0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parse = json.loads(persed_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "491b8f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dic_parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bcc93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa7cc0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_dics = dic_parse['skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ed8fce6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(skill_dics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2363badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'TensorFlow', 'Scikit-Learn', 'Pandas', 'NumPy', 'Matplotlib', 'Seaborn', 'Tableau', 'PostgreSQL', 'Linear Regression', 'Random Forest Regressor', 'KNN', 'Naive Bayes', 'SVM', 'Decision Trees', 'Random Forest Classifier', 'Bagging', 'XgBoost', 'K-Means', 'DBSCAN', 'Natural Language Processing', 'Deep Learning', 'Flask', 'NLTK', 'Streamlit', 'Word2Vec', 'TF-IDF', 'Grid Search', 'Cosine Similarity', 'Bag of Words']\n"
     ]
    }
   ],
   "source": [
    "print(skill_dics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb1d97e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'TensorFlow',\n",
       " 'Scikit-Learn',\n",
       " 'Pandas',\n",
       " 'NumPy',\n",
       " 'Matplotlib',\n",
       " 'Seaborn',\n",
       " 'Tableau',\n",
       " 'PostgreSQL',\n",
       " 'Flask',\n",
       " 'Streamlit',\n",
       " 'NLTK']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lia_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ce9f467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lia_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a2790c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = parsed_resume_skils(text, prompt_template, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0089fc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'Pandas',\n",
       " 'NumPy',\n",
       " 'Matplotlib',\n",
       " 'Seaborn',\n",
       " 'Tableau',\n",
       " 'Scikit-Learn',\n",
       " 'NLTK',\n",
       " 'Flask',\n",
       " 'Streamlit',\n",
       " 'PostgreSQL',\n",
       " 'TensorFlow']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bddb0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "196d1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b19328bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"job_vector_db\",  # Path to saved folder\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7d6a48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \"\n",
    "\n",
    "for i,doc in enumerate(vector_store.docstore._dict.values()):\n",
    "    text += doc.page_content\n",
    "    if i==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_jd(docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the first half of job description text from a list of Document objects.\n",
    "\n",
    "    The function concatenates the `page_content` from the first half of the documents\n",
    "    and returns it as a single text string.\n",
    "\n",
    "    Args:\n",
    "        docs (List[Document]): A list of Document objects containing job descriptions.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated text from the first half of the documents.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a list or is empty.\n",
    "        AttributeError: If an item in the list does not contain 'page_content'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not docs or not isinstance(docs, list):\n",
    "            raise ValueError(\"Input must be a non-empty list of Document objects.\")\n",
    "\n",
    "        text = \"\"\n",
    "        mid = len(docs) // 2\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            if not hasattr(doc, \"page_content\"):\n",
    "                raise AttributeError(f\"Document at index {i} is missing 'page_content'.\")\n",
    "            text += doc.page_content\n",
    "            if i == mid:\n",
    "                break\n",
    "\n",
    "        logger.info(f\"Extracted text from {mid + 1} documents out of {len(docs)}\")\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from job descriptions: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_courses(jd_skills,resume_skills, embeddings, skill_embeddings, df, top_n=3):\n",
    "    \"\"\"\n",
    "    Optimized course recommendation based on skill gaps with reduced time complexity.\n",
    "    \n",
    "    Args:\n",
    "           \n",
    "        jd_skills: List of skills from job_data         \n",
    "        resume_skills: List of skills from resume (lowercase)\n",
    "        embeddings: Embedding model (same as used in vector store) HuggingFaceEmbeddings\n",
    "        skill_embeddings: Precomputed embeddings for skills\n",
    "        df: DataFrame containing course information\n",
    "        top_n: Number of courses to recommend\n",
    "    \n",
    "    Returns:        \n",
    "        - recommended_courses: List of recommended courses       \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert resume skills to set for O(1) lookups\n",
    "        resume_skills_set = set(skill.lower() for skill in resume_skills)        \n",
    "\n",
    "        # Extract skills from vector store\n",
    "        jd_skills_set = set(skill.lower() for skill in jd_skills)\n",
    "        \n",
    "        skill_gaps = jd_skills_set - resume_skills_set      \n",
    "                \n",
    "        logger.info(f\"Identified skill gaps: {skill_gaps}\")\n",
    "        \n",
    "        if not skill_gaps:\n",
    "            logger.warning(\"No skill gaps found. Returning empty result.\")\n",
    "            return \"No skill gap found, Recommend some course to enhance skill\"\n",
    "\n",
    "        # Embed the skill gap text\n",
    "        skill_gap_text = \" \".join(skill_gaps)\n",
    "        skill_gap_embedding = embeddings.embed_query(skill_gap_text)\n",
    "        logger.info(\"Skill gap embedding generated.\")\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity_scores = cosine_similarity(\n",
    "            [skill_gap_embedding],\n",
    "            skill_embeddings\n",
    "        )[0]\n",
    "\n",
    "        # Get top N course indices\n",
    "        top_indices = np.argpartition(similarity_scores, -top_n)[-top_n:]\n",
    "        top_indices = top_indices[np.argsort(similarity_scores[top_indices])[::-1]]\n",
    "\n",
    "        # Prepare course recommendation results\n",
    "        recommended_courses = []\n",
    "        for idx in top_indices:\n",
    "            course = {\n",
    "                \"title\": df.iloc[idx].get('Title', 'N/A'),\n",
    "                \"organization\": df.iloc[idx].get('Organization', 'N/A'),\n",
    "                \"platform\": df.iloc[idx].get('Platform', 'N/A')                \n",
    "            }\n",
    "            recommended_courses.append(course)\n",
    "\n",
    "        logger.info(\"Top recommended courses generated.\")\n",
    "\n",
    "        return recommended_courses            \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in recommend_courses: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "24443943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12178"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d9276ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TransUnion's Job Applicant Privacy Notice\n",
      "\n",
      "What We'll Bring:\n",
      "\n",
      "This position is responsible for supporting the development of credit risk management and business intelligence analytic solutions through consulting engagements and research serving TransUnion’s clients.\n",
      "What You'll Bring:\n",
      "\n",
      "What we’ll bring:\n",
      "  \n",
      "\n",
      "* A work environment that encourages collaboration and innovation. We consistently explore new technologies and tools to be agile.\n",
      "* Flexible time off, workplace flexibility, an environment that welcomes continued professional growth through support of tuition reimbursement, conferences and seminars.\n",
      "* Our culture encourages our people to hone current skills and build new capabilities while discovering their genius.\n",
      "* We provide a modern computing environment based on best\\-in\\-class \"big data\" and cloud computing technologies and the freedom to explore new data sources and statistical and machine learning methodologies.\n",
      "* Our Analytics team is home to some of the most brilliant minds in the market. Here, we will not only understand your stats jokes, we’ll appreciate them.\n",
      "\n",
      "  \n",
      "\n",
      "What you’ll bring:What you’ll bring:\n",
      "  \n",
      "\n",
      "* Bachelors (4 year) degree in statistics, applied mathematics, financial mathematics, engineering, operations research, or another highly quantitative field. And, at least four (6\\) year of professional experience performing analytic work in Financial Services or related industries\n",
      "* Strong analytical, critical thinking, and creative problem\\-solving skills\n",
      "* Excellent understanding of machine learning techniques and algorithms, such as Classification, Regression, Clustering, Feature Engineering, Decision Trees, Gradient Boosting, etc.\n",
      "* Advanced programming skills; proficiency with a statistical language such as R; experience using other programming and data manipulation languages preferred (SQL, Hive, Pig, Python, C/C\\+\\+, Java); high level of familiarity with Microsoft Office tools\n",
      "* Versatile interpersonal and communication style with the ability to effectively communicate at multiple levels within and outside the organization; ability to work in a collaborative, fast\\-paced environment\n",
      "* Strong project management skills with the ability to manage multiple assignments effectively\n",
      "* Ability to travel 10\\-20%\n",
      "\n",
      "  \n",
      "\n",
      "What we'd prefer to see:What we'd prefer to see:\n",
      "  \n",
      "\n",
      "* Master’s (2 years) or PhD degree in statistics, applied mathematics, financial mathematics, engineering, operations research, or another highly quantitative field. And, at least one (1\\) year of professional experience performing analytic work in Financial Services or related industries\n",
      "* Familiarity with credit bureau data and business practices\n",
      "* Experienced with Tableau or other visualization tools\n",
      "* Advanced skills using Excel formulas, macros, and pivot tables to provide detailed analytical reports.\n",
      "* Operates under modest supervision in a complex and dynamic, matrixed environment\n",
      "* Experienced working in a company with a global presence\n",
      "* Experience with modern \"big data\" frameworks (Hadoop, Spark, cloud)\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "Impact you’ll make:\n",
      "  \n",
      "\n",
      "This position is responsible for supporting the development of credit risk management and business intelligence analytic solutions through consulting engagements and research serving TransUnion’s clients.* You will partner with internal and external cross\\-functional teams to drive new business initiatives and deliver long term value\\-added product propositions for TU’s B2B customers globally. This includes but is not limited to developing predictive risk management and business intelligence solutions for credit card issuers, auto \\& mortgage lenders, collections agencies, and retail banks.\n",
      "* You will contribute in analytic engagements involving descriptive, predictive, and prescriptive analysis through the consumer lending portfolio lifecycle, leveraging various techniques (e.g., segmentation, logistic regression, survival analysis, principal component analysis, Monte Carlo simulation, scenario and sensitivity analysis).\n",
      "* You will design and write programs for data extraction, segmentation and statistical analysis on large population datasets using languages such as R, Python, SQL, Hive, and Spark on server and cloud based computing platforms.\n",
      "* You will deliver analytic insights and recommendations in succinct and compelling presentations for internal and external customers and an executive audience.\n",
      "* You will identify strategies and opportunities for customers to test and adopt TransUnion’s analytic products and services.\n",
      "* You will foster a high performance culture and cultivate an environment that promotes excellence and reflects the TransUnion brand.\n",
      "Impact You'll Make:We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law.\n",
      "\n",
      "This is a hybrid position and involves regular performance of job responsibilities virtually as well as in\\-person at an assigned TU office location for a minimum of two days a week.\n",
      "TransUnion Job Title\n",
      "\n",
      "Consultant, Data Science and AnalyticsLTIMindtree is looking for Data Scientist Role\n",
      " \n",
      "\n",
      " Experience\\- 5 to 8 Years\n",
      " \n",
      "\n",
      " Job Location\\- Pune\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " Data Scientist\n",
      " \n",
      "\n",
      " Strong problemsolving skills and ability to deliver highquality datafocused solutions\n",
      " \n",
      "\n",
      " Proficiency in Python SQL and expertise in machine learning and deep learning algorithms\n",
      " \n",
      "\n",
      " Experience with generative AI and LangChain for building and deploying advanced models\n",
      " \n",
      "\n",
      " Proficiency in Azure cloud services for implementing and managing AI solutions\n",
      " \n",
      "\n",
      " Drive the development of impactful data products to support business objectives\n",
      " \n",
      "\n",
      " Skills\\-Machine Learning, Deep Learing, Python, Pytorch, Cloud,AI, Gen AI exp mandatory\n",
      " \n",
      "\n",
      " If interested please share below details on darshana.potdukhe@ltimindtree.com\n",
      " \n",
      "\n",
      " Total Experience:\n",
      " \n",
      "\n",
      " Current CTC:\n",
      " \n",
      "\n",
      " Expected CTC:\n",
      " \n",
      "\n",
      " Official NP: (If serving last working day)Job brief \n",
      "\n",
      " We are looking for a passionate certified Data Analyst. The successful candidate will turn data into information, information into insight and insight into business decisions.\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " As the Lead \\- Analytics, you will lead our strategic efforts to leverage data in alignment with our business objectives. Your role will be pivotal in fostering a culture of analytics, implementing best practices, and improving data\\-driven decision making. You will align and direct the development, management, and integration of our analytics and business intelligence (BI) capability to support the mission, vision, objectives, and goals of the overall business. Additionally, we seek someone who comprehensively understands how the business operates as a whole, recognizing the importance of analytics in each department to paint a cohesive and comprehensive picture.\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "Essential Responsibilities: \n",
      "\n",
      " • Strategic Leadership: Drive a culture of Business Intelligence by assisting in architecting and executing a visionary analytics strategy that harnesses our data ecosystem. Responsible for driving departmental adoption of analytics while promoting data\\-driven decision\\-making and customer\\- and consumer\\-driven insights.• Excellent Communication \\& Coordination: Since the position will primarily be coordinating \\& working with our U.S office team, hence excellent command over verbal \\& written English is an important pre\\-requisite for the position. Ideal candidate should be open for working as per US shift timings.\n",
      " \n",
      "\n",
      " • Analytics Infrastructure: Collaborate with IT Architecture Leadership around the development and maintenance of robust analytics infrastructure, including data warehouses, reporting tools, and visualization platforms.\n",
      " \n",
      "\n",
      " • Analytics and Modeling: Establish a foundation for deploying advanced statistical models and machine learning algorithms in the future, focusing on building the necessary infrastructure and expertise for predictive analysis and optimization.\n",
      " \n",
      "\n",
      " • Dashboard and KPI Management: Develop dashboards and KPIs to drive decision\u0002making processes.\n",
      " \n",
      "\n",
      " • Collaboration and Influence: Collaborate with key stakeholders across departments to understand business needs and translate them into data and analytics requirements. Act as a bridge between technical teams and business units, translating complex data insights into actionable business strategies. Effective communication with both technical and non\\-technical team members.\n",
      " \n",
      "\n",
      " • Legacy: Comfortable with creating foundational data strategy that will enable a hyper growth strategy.• Legacy: Comfortable with creating foundational data strategy that will enable a hyper growth strategy.\n",
      " \n",
      "\n",
      " • Data Governance: Ensure data integrity and consistency across all reporting, documentation of data assets and KPIs, establish source of truth for data domains. Implement data quality standards and procedures, overseeing data cleansing and validation processes.\n",
      " \n",
      "\n",
      " • Technical Expertise: Utilize SSIS for ETL processes, ensuring seamless data integration and automation across systems. Employ Power BI for comprehensive business intelligence solutions, creating dynamic reports and dashboards that drive decision\\-making\n",
      " \n",
      "\n",
      " • Continuous Improvement: Stay informed about industry trends and emerging technologies in data and analytics. Identify opportunities for process optimization and the application of advanced analytics techniques and build a 3\\-year roadmap for data analytics.\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "Skills \\& Minimum Qualifications: \n",
      " To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of knowledge, skill, and/or ability required. Reasonable accommodation may be made to enable individuals with disabilities to perform essential functions.\n",
      " \n",
      "\n",
      " • Bachelors or Master's in Data Science, Computer Science, or related fields. • At least 7 years of experience in a leadership role within analytics/data science, specifically within the retail or ecommerce sector.• Working knowledge in data management and analytics platforms, such as SQL Server, SSIS, Power BI, and Google Analytics. • Demonstrated ability in statistical analysis, predictive modeling, and machine learning techniques.\n",
      " \n",
      "\n",
      " • Demonstrated ability to analyze and interpret trends in large datasets, identifying patterns and insights that drive strategic decision\\-making.\n",
      " \n",
      "\n",
      " • Familiarity with ERP systems and their application in data\\-driven business environments.\n",
      " \n",
      "\n",
      " • Experience in implementing tailored solutions to improve customer journey mapping, satisfaction metrics, and brand loyalty.\n",
      " \n",
      "\n",
      " • Proficiency in warehouse management tools, demonstrating expertise in utilizing data analytics to optimize supply chain operations and enhance efficiency within warehouse management systems.\n",
      " \n",
      "\n",
      " • Ability to communicate trends and their implications clearly and concisely to stakeholders at all levels of the organization, informing strategic planning and resource allocation.\n",
      " \n",
      "\n",
      " • Strong project management skills, presentation skills, and a successful track record of leading successful cross\\-functional initiatives.\n",
      " \n",
      "\n",
      " • Demonstrated track record of working with consumer brand initiatives or enhancing customer experiences through data\\-driven strategies.\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "Specialized Training or Knowledge: \n",
      "\n",
      " • Proficiency with Microsoft Office applications, with expertise in Excel (e.g., pivot tables, advanced functions, formulas, filtering, etc.) and database skills (e.g., SQL)• Proficiency with Microsoft Office applications, with expertise in Excel (e.g., pivot tables, advanced functions, formulas, filtering, etc.) and database skills (e.g., SQL)\n",
      " \n",
      "\n",
      " • Ability to collect and synthesize information, making it relevant, understandable, and actionable for key stakeholders\n",
      " \n",
      "\n",
      " • Ability to balance multiple projects with competing deadlines\n",
      " \n",
      "\n",
      " • Generate insights that improve the business through linking various data sources\n",
      " \n",
      "\n",
      " • Strong understanding of Analytics and Visualization techniques\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8bed51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = text[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "04ff45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = parsed_resume_skils(text, prompt_template, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "85931666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'R',\n",
       " 'SQL',\n",
       " 'Hive',\n",
       " 'Pig',\n",
       " 'C/C++',\n",
       " 'Java',\n",
       " 'Microsoft Office',\n",
       " 'Tableau',\n",
       " 'Excel',\n",
       " 'Machine Learning',\n",
       " 'Deep Learning',\n",
       " 'Azure',\n",
       " 'Cloud',\n",
       " 'Gen AI',\n",
       " 'LangChain',\n",
       " 'Pytorch',\n",
       " 'SSIS',\n",
       " 'Power BI',\n",
       " 'Google Analytics',\n",
       " 'SQL Server',\n",
       " 'ERP',\n",
       " 'Warehouse Management',\n",
       " 'Pivot Tables',\n",
       " 'Database Skills',\n",
       " 'Data Visualization',\n",
       " 'Statistical Analysis',\n",
       " 'Predictive Modeling',\n",
       " 'Gradient Boosting',\n",
       " 'Decision Trees',\n",
       " 'Classification',\n",
       " 'Regression',\n",
       " 'Clustering',\n",
       " 'Feature Engineering',\n",
       " 'Hadoop',\n",
       " 'Spark',\n",
       " 'Big Data',\n",
       " 'Data Science',\n",
       " 'Computer Science',\n",
       " 'Data Management',\n",
       " 'Analytics',\n",
       " 'Business Intelligence',\n",
       " 'Data Governance',\n",
       " 'Data Quality',\n",
       " 'Data Warehousing',\n",
       " 'ETL',\n",
       " 'Data Mining',\n",
       " 'Data Visualization Tools',\n",
       " 'Statistical Languages',\n",
       " 'Programming Languages',\n",
       " 'Cloud Computing',\n",
       " 'Artificial Intelligence']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f6919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba378274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
