{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "welcome\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('welcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv('RAPIDAPI_KEY')\n",
    "\n",
    "INDIAN_CITIES = [\n",
    "    \"Mumbai\", \"Delhi\", \"Bangalore\", \"Hyderabad\", \"Ahmedabad\",\n",
    "    \"Chennai\", \"Kolkata\", \"Surat\", \"Pune\", \"Jaipur\",\n",
    "    \"Lucknow\", \"Kanpur\", \"Nagpur\", \"Visakhapatnam\", \"Indore\",\n",
    "    \"Thane\", \"Bhopal\", \"Patna\", \"Vadodara\", \"Ghaziabad\"\n",
    "]\n",
    "\n",
    "def fetch_jobs(query, location=\"India\", results_wanted=5,api_key=api_key):\n",
    "    conn = http.client.HTTPSConnection(\"jobs-search-api.p.rapidapi.com\")\n",
    "    \n",
    "    # If location is \"India\", use random cities\n",
    "    if location.lower() == \"india\":\n",
    "        # Calculate how many jobs per city (at least 1 city per job)\n",
    "        jobs_per_city = max(1, results_wanted // len(INDIAN_CITIES))\n",
    "        all_jobs = []\n",
    "        \n",
    "        for city in random.sample(INDIAN_CITIES, min(len(INDIAN_CITIES), results_wanted)):\n",
    "            payload = json.dumps({\n",
    "                \"search_term\": query,\n",
    "                \"location\": f\"{city}, India\",\n",
    "                \"results_wanted\": jobs_per_city,\n",
    "                \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "                \"distance\": 50,\n",
    "                \"job_type\": \"fulltime\",\n",
    "                \"is_remote\": False,\n",
    "                \"linkedin_fetch_description\": True,\n",
    "                \"hours_old\": 72\n",
    "            })\n",
    "\n",
    "            headers = {\n",
    "    'x-rapidapi-key': \"ab8248b873msh2242a61781bb598p1262a9jsn98ab57e85059\",\n",
    "    'x-rapidapi-host': \"jobs-search-api.p.rapidapi.com\",\n",
    "    'Content-Type': \"application/json\"\n",
    "}\n",
    "\n",
    "            try:\n",
    "                conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "                res = conn.getresponse()\n",
    "                data = res.read().decode(\"utf-8\")\n",
    "                city_jobs = json.loads(data).get(\"jobs\", [])\n",
    "                \n",
    "                # Add city information to each job\n",
    "                for job in city_jobs:\n",
    "                    job[\"searched_location\"] = city\n",
    "                all_jobs.extend(city_jobs)\n",
    "                \n",
    "                # Stop if we've collected enough jobs\n",
    "                if len(all_jobs) >= results_wanted:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching jobs for {city}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # Trim to exact result count and format\n",
    "        return [\n",
    "            {\n",
    "                \"job title\": job[\"title\"],\n",
    "                \"company\": job[\"company\"],\n",
    "                \"location\": job.get(\"location\", \"N/A\"),\n",
    "                \"searched_city\": job.get(\"searched_location\", \"India\"),\n",
    "                \"description\": job[\"description\"]\n",
    "            }\n",
    "            for job in all_jobs[:results_wanted]\n",
    "            if all(key in job for key in [\"title\", \"company\", \"description\"])\n",
    "        ]\n",
    "    \n",
    "    else:\n",
    "        # Original single-location logic\n",
    "        payload = json.dumps({\n",
    "            \"search_term\": query,\n",
    "            \"location\": location,\n",
    "            \"results_wanted\": results_wanted,\n",
    "            \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "            \"distance\": 50,\n",
    "            \"job_type\": \"fulltime\",\n",
    "            \"is_remote\": False,\n",
    "            \"linkedin_fetch_description\": True,\n",
    "            \"hours_old\": 72,\n",
    "            \"show_requirements\": True, \n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "    'x-rapidapi-key': \"ab8248b873msh2242a61781bb598p1262a9jsn98ab57e85059\",\n",
    "    'x-rapidapi-host': \"jobs-search-api.p.rapidapi.com\",\n",
    "    'Content-Type': \"application/json\"\n",
    "}\n",
    "\n",
    "        conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "        res = conn.getresponse()\n",
    "        data = res.read().decode(\"utf-8\")\n",
    "        job_data = json.loads(data)\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"job title\": job[\"title\"],\n",
    "                \"company\": job[\"company\"],\n",
    "                \"location\": job.get(\"location\", \"N/A\"),\n",
    "                \"searched_city\": location.split(\",\")[0].strip(),\n",
    "                \"description\": job[\"description\"]\n",
    "            }\n",
    "            for job in job_data.get(\"jobs\", [])\n",
    "            if all(key in job for key in [\"title\", \"company\", \"description\"])\n",
    "        ]\n",
    "        \n",
    "def clean_text(text):\n",
    "    \"\"\"Remove excessive newlines and markdown bold syntax\"\"\"\n",
    "    text = re.sub(r'\\*\\*', '', text)  # Remove **bold** markers\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Replace 3+ newlines with double newlines\n",
    "    return text.strip()\n",
    "\n",
    "def documentation(job_details):\n",
    "    content=[] \n",
    "    for job in job_details: \n",
    "        doc = Document(\n",
    "                page_content=clean_text(job[\"description\"]),\n",
    "                metadata={\n",
    "                    \"job_title\": job[\"job title\"],\n",
    "                    \"company\": job[\"company\"],\n",
    "                    \"location\": job[\"location\"],\n",
    "                    \"searched_city\": job[\"searched_city\"],\n",
    "                    \n",
    "                    \"language\": \"en\"\n",
    "                    }\n",
    "                )\n",
    "        content.append(doc)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs='Data Analyst'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_job=fetch_jobs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'job title': 'AI Data Engineer',\n",
       "  'company': 'Uplers',\n",
       "  'location': 'Kanpur, Uttar Pradesh, India',\n",
       "  'searched_city': 'Kanpur',\n",
       "  'description': \"**Experience** \\n : 3\\\\.00 \\\\+ years\\n   \\n\\n  \\n\\n**Salary** \\n : USD 18000\\\\-30000 / year (based on experience)\\n   \\n\\n  \\n\\n**Expected Notice Period** \\n : 15 Days\\n   \\n\\n  \\n\\n**Shift** \\n : (GMT\\\\+05:30\\\\) Asia/Kolkata (IST)\\n   \\n\\n  \\n\\n**Opportunity Type** \\n : Remote\\n   \\n\\n  \\n\\n**Placement Type** \\n : Full Time Indefinite Contract(40 hrs a week/160 hrs a month)\\n   \\n\\n  \\n\\n**(\\\\*Note: This is a requirement for one of Uplers' client \\\\- Steer Health)\\n   \\n\\n  \\n\\n**What do you need for this opportunity?\\n   \\n\\n  \\n\\n**Must have skills required:**\\n Airflow, Kubeflow, LangChain, RAGFlow, TensorFlow, Dialogflow, FastAPI, LLMs, Pytorch, Python\\n   \\n\\n  \\n\\n**Steer Health is Looking for:******\\n**About The Role**\\n Steer Health is seeking a talented \\\\*\\\\*Backend Engineer\\\\*\\\\* with expertise in AI/ML and healthcare technologies to design and implement \\\\*\\\\*AgenticAI workflows\\\\*\\\\* that redefine clinical and operational processes. You’ll build scalable backend systems that integrate FHIR\\\\-compliant APIs, LLM\\\\-driven automation, and conversational AI to solve real\\\\-world healthcare challenges. If you’re passionate about Python, AI workflows, and making a tangible impact in healthcare, this role is for you.\\n   \\n\\n  \\n\\n Key Responsibilities\\n   \\n\\n  \\n\\n* FastAPI to enable seamless data exchange across EHRs, patient portals, and AI agents.\\n* Architect AI\\\\-driven workflows using tools like RAGFlow or similar platforms to automate tasks such as clinical documentation, prior authorization, and patient triage.\\n* Develop and fine\\\\-tune LLM\\\\-based solutions (e.g., GPT, Claude) with PyTorch, focusing on healthcare\\\\-specific use cases like diagnosis support or patient communication.\\n* Integrate Dialogflow for conversational AI agents that power chatbots, voice assistants, and virtual health aides.\\n* Collaborate on prompt engineering to optimize LLM outputs for accuracy, compliance, and clinical relevance.\\n* Optimize backend systems for performance, scalability, and security in HIPAA\\\\-compliant environments.\\n* Partner with cross\\\\-functional teams (data scientists, product managers, clinicians) to translate healthcare needs into technical solutions.\\n\\n\\n**Qualifications**\\n* 3\\\\+ years of backend engineering experience, with expertise in Python and frameworks like FastAPI or Flask.\\n* Hands\\\\-on experience with \\\\*\\\\*PyTorch/TensorFlow\\\\*\\\\* and deploying ML models in production.\\n* Familiarity with AI workflow tools (e.g., RAGFlow, Airflow, Kubeflow) and orchestration of LLM pipelines.\\n* Experience integrating Dialogflow or similar platforms for conversational AI.\\n* Strong understanding of LLMs (training, fine\\\\-tuning, and deployment) and prompt engineering best practices.\\n* Knowledge of cloud platforms (AWS/GCP/Azure) and containerization (Docker, Kubernetes).\\n* Passion for healthcare innovation and improving patient/provider experiences.\\n\\n\\n**Preferred Qualifications**\\n* Experience in healthcare tech (EHR integrations, HIPAA compliance, HL7/FHIR).\\n* Contributions to open\\\\-source AI/healthcare projects.\\n* Familiarity with \\\\*\\\\*LangChain\\\\*\\\\*, \\\\*\\\\*LlamaIndex\\\\*\\\\*, or agentic workflow frameworks.\\n\\n\\n Why Join Steer Health?\\n   \\n\\n  \\n\\n* Impact: Your work will directly enhance healthcare delivery for millions of patients.\\n* Innovation: Build with the latest AI/ML tools in a fast\\\\-paced, forward\\\\-thinking environment.\\n* Growth: Lead projects at the intersection of AI and healthcare, with opportunities for advancement.\\n* Culture: Collaborative, mission\\\\-driven team with flexible work policies.\\n\\n\\n**How to apply for this opportunity?**\\n* Step 1: Click On Apply! And Register or Login on our portal.\\n* Step 2: Complete the Screening Form \\\\& Upload updated Resume\\n* Step 3: Increase your chances to get shortlisted \\\\& meet the client for the Interview!\\n\\n\\n**About Uplers:**\\n Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement.\\n   \\n\\n  \\n\\n (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).\\n   \\n\\n  \\n\\n So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!\"},\n",
       " {'job title': 'Analyst,SC Advanced Analytics - Data & Insights',\n",
       "  'company': 'General Mills',\n",
       "  'location': 'India',\n",
       "  'searched_city': 'Thane',\n",
       "  'description': ':\\n\\nIndia is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R\\\\&D \\\\& Quality), and Enterprise Business Services. Learning and capacity\\\\-building is a key ingredient of our success.\\nJob Description: **Position Title**\\n\\n  \\n\\nAnalyst, SC Adv Analytics – Data and Insights\\n\\n **Function/Group**\\n\\n  \\n\\nSC Adv Analytics\\n\\n **Location**\\n\\n  \\n\\nMumbai\\n\\n **Shift Timing**\\n\\n **1\\\\.30pm to 10\\\\.30pm**\\n\\n **Role Reports to**\\n\\n  \\n\\nAssistant Manager, SC Adv Analytics – Data and Insights\\n\\n **Remote/Hybrid/in\\\\-Office**\\n\\n  \\n\\nHybrid\\n\\n **ABOUT GENERAL MILLS****We make food the world loves: 100 brands. In 100 countries. Across six continents.** With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen\\\\-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com  \\n\\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow. **JOB OVERVIEW**\\nSC Reporting and Analytics group is responsible for development of the reports, tools \\\\& dashboards which support and aid in decision making process in Supply Chain. This team provides exposure to Demand Planning, Supply Planning, Deployment, Transportation, Inbound Logistics and Sourcing function.\\nThis team works directly with Supply Chain, Business teams and IT for developing solutions which aid decision making and drive business value.\\nThis team drives step\\\\-change innovation and improvements in business practices by delivering actionable insights through advanced analytics and Supply chain expertise. **KEY ACCOUNTABILITIES*** Participate in connects with stakeholders during project requirements gathering, updates, feedback gathering, stand\\\\-ups etc.\\n* Data and data sources exploration across multiple data sources – Data Lake, Oracle, SQL, AFO, SAP etc.\\n* Perform complex ETL (Extract\\\\-Transform\\\\-Load) operation on large data sets, Execute efficient data transformation techniques depending on the tools.\\n* Create new reports / Tools / Automations /dashboards for business stakeholders, that drive insights and enable better decision making\\n* Deliver time savings through internal process automations\\n* Should possess strong knowledge of Tableau architecture (design, development), SQL, query optimization, and end user experience\\n* Should be able to understand and navigate through the back\\\\-end data architecture and troubleshoot data issues if any\\n* Ensure project related documentation is updated\\n* Ensure data accuracy and quality\\n* Support short term capabilities sustainance.\\n* Keep project notes and projects document updated and verified\\n* Support On\\\\-boarding of new team members\\n* Conduct basic on boarding trainings\\n* Proactive Participation in Technical, Soft Skill Training\\n* Participate/Lead other engagement activities across SC Analytics team.\\n* Demonstrate Effective written and verbal Communication\\n\\n **MINIMUM QUALIFICATIONS**\\n\\\\-Education – Full time graduation from an accredited university (Mandatory\\\\- Note: This is the minimum education criteria which cannot be altered)**\\\\-Graduation in \\\\-** **BSc./****MSc., B.E.****, B.Tech.** **, Engineering, MBA Operations/SC, BCA/MCA*** Specific Job Experience or Skills Needed:\\n* SQL – Intermediate Queries, Procedures\\n* Advanced MS Excel – Pivots, Array Formulas, Formulas (E.g.: SUMIFS; VLOOKUP; HLOOKUP; MAXIF etc), Conditional Formatting and other concepts.\\n* VBA – Functions, Procedures, Macro Development, Error Handling\\n* Visualization –Tableau Dashboard Development\\n* Project Management – Basic at execution level. Analytical and prioritization skills\\n* Problem solving skill\\n\\n**Competencies/Behaviors** **required** **for job:*** Agility\\n* Understands quality and strives to deliver on time.\\n* Thrives in working on couple of projects at one time.\\n* Has curiosity to learn\\n* Works with less / minimum supervision\\n* Executes one key expertise independently\\n* Deliver Results:\\n* Assume personal initiative and accountability for results, performance and behaviors.\\n* Be comfortable with ambiguity.\\n\\n **PREFERRED QUALIFICATIONS*** Masters\\n* SQL Certification\\n* Advanced Excel \\\\& VBA Certification\\n* MIS Reporting Certification\\n\\n  \\n\\n\\nCompany Overview:\\n\\nWe exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next.'},\n",
       " {'job title': 'Data Analyst',\n",
       "  'company': \"L'Oréal\",\n",
       "  'location': 'Mumbai, Maharashtra, India',\n",
       "  'searched_city': 'Thane',\n",
       "  'description': \"The world leader in cosmetics, L’Oréal is present in 150 countries on five continents. Our 35 international brands have allowed us to devote ourselves solely to one business: beauty, with a mission to provide the best in cosmetics innovation to all globally. Our ambition is to win over another one billion consumers around the world by inventing the cosmetic products that meet the infinite diversity of their needs and desires through continued digital innovation. L’Oréal supports diversity and sustainable, ethical sourcing for all our products, and we have reduced our emissions by approx. 50% since 2005\\\\.\\n   \\n\\n  \\n\\n L’Oreal has a long history rooted in Responsibility, with\\n *L’Oreal for the Future* \\n program launched in 2020, we have a taken on an even greater responsibility and believe that companies can be part of the solution to the environment challenges the world is facing.\\n   \\n\\n  \\n\\n With our business model transforming towards O\\\\+O, digital transformation is one of the key business drivers. Given the business dynamics and complexities, having the right hold on business data is super critical. There is significant opportunity to develop enterprise\\\\-wide solutions for complex data to derive insights, improve decision making and automate processes.\\n   \\n\\n  \\n\\n In our journey, we are looking for talented individuals who can lead us on this mission.\\n   \\n\\n  \\n\\n Would you like to be a part of the adventure?\\n   \\n\\n  \\n\\n We have an opportunity for the position of\\n **Data Analyst.**\\n As a Data Analyst within our Data \\\\& Analytics Team, you will be a crucial partner to the business. Collaborating closely with our business teams across divisions, you will leverage data to drive strategic decision\\\\-making and optimize operational efficiency.\\n   \\n\\n  \\n\\n**Job Mission:** \\n Assemble and transform data into information and insights to facilitate business strategy and decision making, leveraging state of the art platforms, visualization and analytics techniques within an entity or function.\\n   \\n\\n  \\n\\n The\\n **location o** \\n f the job will be\\n **Mumbai** \\n and this role reports into Data Product Owner.\\n   \\n\\n  \\n\\n**Key Job Responsibilities**\\n* Data analysis: Answer crucial business questions by analyzing the data in our data base, identify potential gaps and opportunities in our data. Validate the data accuracy vs the source systems or business expectations to guarantee our outputs to be accurate.\\n* Ad Hoc analysis: Accelerate opportunities in the business by proactively working with the business stakeholders to test and validate hypotheses and root cause analysis to drive business decisions\\n* Data Architecture \\\\& Visualization: Manage the data architecture, aggregating data from multiple sources. Develop and maintain automated dashboards and visualization templates using BI software.\\n* Requirements Gathering \\\\& Reporting: Collaborate with various functions and business entities to identify and define reporting and metric requirements. Develop new reports and dashboards to meet these needs.\\n* Business Partnership: Provide data\\\\-driven insights and analyses to support business teams in making informed decisions.\\n* Training \\\\& Upskilling: Provide training on new functionalities and tools to teams, fostering data literacy within the business teams.\\n\\n\\n**Profile \\\\& Experience**\\n* Bachelor's degree or higher in Information Management, Statistics, Mathematics, Computer Science, or a related field.\\n* 3\\\\-5 years of experience in data management or business analysis, preferably with a focus on functional business topics.\\n* Strong planning, organizational, and prioritization skills.\\n* A solutions\\\\-oriented mindset with the ability to work accurately and efficiently.\\n* Meticulous attention to detail.\\n* Experience contributing to key business projects.\\n* Strong analytical and problem\\\\-solving skills, with the ability to identify root causes and implement process improvements.\\n* Proactive and takes initiative, contributing ideas and taking ownership of responsibilities.\\n\\n\\n**Physical Demands (e.g. % travel):**\\n* Travel will be need based\\n\\n\\n L’Oréal is committed to building a diverse environment and is proud to be an equal opportunity employer. L’Oréal closely prohibits discrimination against any employee or applicant for employment because of the individual’s race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability or any other characteristic protected by law.\"},\n",
       " {'job title': 'Planning Engineer',\n",
       "  'company': 'BE Realty',\n",
       "  'location': 'Lucknow, Uttar Pradesh, India',\n",
       "  'searched_city': 'Lucknow',\n",
       "  'description': 'As in my other post'},\n",
       " {'job title': 'AI Data Engineer',\n",
       "  'company': 'Uplers',\n",
       "  'location': 'Visakhapatnam, Andhra Pradesh, India',\n",
       "  'searched_city': 'Visakhapatnam',\n",
       "  'description': \"**Experience** \\n : 3\\\\.00 \\\\+ years\\n   \\n\\n  \\n\\n**Salary** \\n : USD 18000\\\\-30000 / year (based on experience)\\n   \\n\\n  \\n\\n**Expected Notice Period** \\n : 15 Days\\n   \\n\\n  \\n\\n**Shift** \\n : (GMT\\\\+05:30\\\\) Asia/Kolkata (IST)\\n   \\n\\n  \\n\\n**Opportunity Type** \\n : Remote\\n   \\n\\n  \\n\\n**Placement Type** \\n : Full Time Indefinite Contract(40 hrs a week/160 hrs a month)\\n   \\n\\n  \\n\\n**(\\\\*Note: This is a requirement for one of Uplers' client \\\\- Steer Health)\\n   \\n\\n  \\n\\n**What do you need for this opportunity?\\n   \\n\\n  \\n\\n**Must have skills required:**\\n Airflow, Kubeflow, LangChain, RAGFlow, TensorFlow, Dialogflow, FastAPI, LLMs, Pytorch, Python\\n   \\n\\n  \\n\\n**Steer Health is Looking for:******\\n**About The Role**\\n Steer Health is seeking a talented \\\\*\\\\*Backend Engineer\\\\*\\\\* with expertise in AI/ML and healthcare technologies to design and implement \\\\*\\\\*AgenticAI workflows\\\\*\\\\* that redefine clinical and operational processes. You’ll build scalable backend systems that integrate FHIR\\\\-compliant APIs, LLM\\\\-driven automation, and conversational AI to solve real\\\\-world healthcare challenges. If you’re passionate about Python, AI workflows, and making a tangible impact in healthcare, this role is for you.\\n   \\n\\n  \\n\\n Key Responsibilities\\n   \\n\\n  \\n\\n* FastAPI to enable seamless data exchange across EHRs, patient portals, and AI agents.\\n* Architect AI\\\\-driven workflows using tools like RAGFlow or similar platforms to automate tasks such as clinical documentation, prior authorization, and patient triage.\\n* Develop and fine\\\\-tune LLM\\\\-based solutions (e.g., GPT, Claude) with PyTorch, focusing on healthcare\\\\-specific use cases like diagnosis support or patient communication.\\n* Integrate Dialogflow for conversational AI agents that power chatbots, voice assistants, and virtual health aides.\\n* Collaborate on prompt engineering to optimize LLM outputs for accuracy, compliance, and clinical relevance.\\n* Optimize backend systems for performance, scalability, and security in HIPAA\\\\-compliant environments.\\n* Partner with cross\\\\-functional teams (data scientists, product managers, clinicians) to translate healthcare needs into technical solutions.\\n\\n\\n**Qualifications**\\n* 3\\\\+ years of backend engineering experience, with expertise in Python and frameworks like FastAPI or Flask.\\n* Hands\\\\-on experience with \\\\*\\\\*PyTorch/TensorFlow\\\\*\\\\* and deploying ML models in production.\\n* Familiarity with AI workflow tools (e.g., RAGFlow, Airflow, Kubeflow) and orchestration of LLM pipelines.\\n* Experience integrating Dialogflow or similar platforms for conversational AI.\\n* Strong understanding of LLMs (training, fine\\\\-tuning, and deployment) and prompt engineering best practices.\\n* Knowledge of cloud platforms (AWS/GCP/Azure) and containerization (Docker, Kubernetes).\\n* Passion for healthcare innovation and improving patient/provider experiences.\\n\\n\\n**Preferred Qualifications**\\n* Experience in healthcare tech (EHR integrations, HIPAA compliance, HL7/FHIR).\\n* Contributions to open\\\\-source AI/healthcare projects.\\n* Familiarity with \\\\*\\\\*LangChain\\\\*\\\\*, \\\\*\\\\*LlamaIndex\\\\*\\\\*, or agentic workflow frameworks.\\n\\n\\n Why Join Steer Health?\\n   \\n\\n  \\n\\n* Impact: Your work will directly enhance healthcare delivery for millions of patients.\\n* Innovation: Build with the latest AI/ML tools in a fast\\\\-paced, forward\\\\-thinking environment.\\n* Growth: Lead projects at the intersection of AI and healthcare, with opportunities for advancement.\\n* Culture: Collaborative, mission\\\\-driven team with flexible work policies.\\n\\n\\n**How to apply for this opportunity?**\\n* Step 1: Click On Apply! And Register or Login on our portal.\\n* Step 2: Complete the Screening Form \\\\& Upload updated Resume\\n* Step 3: Increase your chances to get shortlisted \\\\& meet the client for the Interview!\\n\\n\\n**About Uplers:**\\n Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement.\\n   \\n\\n  \\n\\n (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).\\n   \\n\\n  \\n\\n So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=documentation(fetch_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'job_title': 'Consultant (Credit Risk Modelling), Data Science & Analytics', 'company': 'TransUnion', 'location': 'Pune', 'searched_city': 'Pune', 'language': 'en'}, page_content='TransUnion\\'s Job Applicant Privacy Notice\\n\\nWhat We\\'ll Bring:\\n\\nThis position is responsible for supporting the development of credit risk management and business intelligence analytic solutions through consulting engagements and research serving TransUnion’s clients.\\nWhat You\\'ll Bring:\\n\\nWhat we’ll bring:\\n  \\n\\n* A work environment that encourages collaboration and innovation. We consistently explore new technologies and tools to be agile.\\n* Flexible time off, workplace flexibility, an environment that welcomes continued professional growth through support of tuition reimbursement, conferences and seminars.\\n* Our culture encourages our people to hone current skills and build new capabilities while discovering their genius.\\n* We provide a modern computing environment based on best\\\\-in\\\\-class \"big data\" and cloud computing technologies and the freedom to explore new data sources and statistical and machine learning methodologies.\\n* Our Analytics team is home to some of the most brilliant minds in the market. Here, we will not only understand your stats jokes, we’ll appreciate them.\\n\\n  \\n\\nWhat you’ll bring:\\n  \\n\\n* Bachelors (4 year) degree in statistics, applied mathematics, financial mathematics, engineering, operations research, or another highly quantitative field. And, at least four (6\\\\) year of professional experience performing analytic work in Financial Services or related industries\\n* Strong analytical, critical thinking, and creative problem\\\\-solving skills\\n* Excellent understanding of machine learning techniques and algorithms, such as Classification, Regression, Clustering, Feature Engineering, Decision Trees, Gradient Boosting, etc.\\n* Advanced programming skills; proficiency with a statistical language such as R; experience using other programming and data manipulation languages preferred (SQL, Hive, Pig, Python, C/C\\\\+\\\\+, Java); high level of familiarity with Microsoft Office tools\\n* Versatile interpersonal and communication style with the ability to effectively communicate at multiple levels within and outside the organization; ability to work in a collaborative, fast\\\\-paced environment\\n* Strong project management skills with the ability to manage multiple assignments effectively\\n* Ability to travel 10\\\\-20%\\n\\n  \\n\\nWhat we\\'d prefer to see:\\n  \\n\\n* Master’s (2 years) or PhD degree in statistics, applied mathematics, financial mathematics, engineering, operations research, or another highly quantitative field. And, at least one (1\\\\) year of professional experience performing analytic work in Financial Services or related industries\\n* Familiarity with credit bureau data and business practices\\n* Experienced with Tableau or other visualization tools\\n* Advanced skills using Excel formulas, macros, and pivot tables to provide detailed analytical reports.\\n* Operates under modest supervision in a complex and dynamic, matrixed environment\\n* Experienced working in a company with a global presence\\n* Experience with modern \"big data\" frameworks (Hadoop, Spark, cloud)\\n\\n  \\n\\n  \\n\\nImpact you’ll make:\\n  \\n\\nThis position is responsible for supporting the development of credit risk management and business intelligence analytic solutions through consulting engagements and research serving TransUnion’s clients.\\n  \\n\\n  \\n\\n* You will partner with internal and external cross\\\\-functional teams to drive new business initiatives and deliver long term value\\\\-added product propositions for TU’s B2B customers globally. This includes but is not limited to developing predictive risk management and business intelligence solutions for credit card issuers, auto \\\\& mortgage lenders, collections agencies, and retail banks.\\n* You will contribute in analytic engagements involving descriptive, predictive, and prescriptive analysis through the consumer lending portfolio lifecycle, leveraging various techniques (e.g., segmentation, logistic regression, survival analysis, principal component analysis, Monte Carlo simulation, scenario and sensitivity analysis).\\n* You will design and write programs for data extraction, segmentation and statistical analysis on large population datasets using languages such as R, Python, SQL, Hive, and Spark on server and cloud based computing platforms.\\n* You will deliver analytic insights and recommendations in succinct and compelling presentations for internal and external customers and an executive audience.\\n* You will identify strategies and opportunities for customers to test and adopt TransUnion’s analytic products and services.\\n* You will foster a high performance culture and cultivate an environment that promotes excellence and reflects the TransUnion brand.\\nImpact You\\'ll Make:\\n\\nWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law.\\n\\nThis is a hybrid position and involves regular performance of job responsibilities virtually as well as in\\\\-person at an assigned TU office location for a minimum of two days a week.\\nTransUnion Job Title\\n\\nConsultant, Data Science and Analytics')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Determine if the following text describes a job role. \n",
    "    Answer strictly 'Yes' or 'No'.\n",
    "    \n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def is_job_role(text):\n",
    "    # Format the prompt with the input text\n",
    "    formatted_prompt = prompt.format(text=text)\n",
    "    \n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_job_role(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1500,chunk_overlap=200)\n",
    "\n",
    "text_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arshi\\Downloads\\Desktop\\Bro-Project\\SkillForge.ai\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding=HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store=FAISS.from_documents(text_chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=embeddings.embed_query(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_sample=vectore_store.as_retriever(search_type='similarity',search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='013abf22-a78a-4004-ac88-a2972a685b24', metadata={'job_title': 'Data Analyst', 'company': 'Jaipur Rugs', 'location': 'Jaipur, Rajasthan, India', 'searched_city': 'Jaipur', 'language': 'en'}, page_content='• Proficiency with Microsoft Office applications, with expertise in Excel (e.g., pivot tables, advanced functions, formulas, filtering, etc.) and database skills (e.g., SQL)\\n \\n\\n • Ability to collect and synthesize information, making it relevant, understandable, and actionable for key stakeholders\\n \\n\\n • Ability to balance multiple projects with competing deadlines\\n \\n\\n • Generate insights that improve the business through linking various data sources\\n \\n\\n • Strong understanding of Analytics and Visualization techniques'),\n",
       " Document(id='dde3b1e9-dc9d-43e1-a53f-a54e1db062f0', metadata={'job_title': 'Data Analyst', 'company': 'Jaipur Rugs', 'location': 'Jaipur, Rajasthan, India', 'searched_city': 'Jaipur', 'language': 'en'}, page_content='• Working knowledge in data management and analytics platforms, such as SQL Server, SSIS, Power BI, and Google Analytics. • Demonstrated ability in statistical analysis, predictive modeling, and machine learning techniques.\\n \\n\\n • Demonstrated ability to analyze and interpret trends in large datasets, identifying patterns and insights that drive strategic decision\\\\-making.\\n \\n\\n • Familiarity with ERP systems and their application in data\\\\-driven business environments.\\n \\n\\n • Experience in implementing tailored solutions to improve customer journey mapping, satisfaction metrics, and brand loyalty.\\n \\n\\n • Proficiency in warehouse management tools, demonstrating expertise in utilizing data analytics to optimize supply chain operations and enhance efficiency within warehouse management systems.\\n \\n\\n • Ability to communicate trends and their implications clearly and concisely to stakeholders at all levels of the organization, informing strategic planning and resource allocation.\\n \\n\\n • Strong project management skills, presentation skills, and a successful track record of leading successful cross\\\\-functional initiatives.\\n \\n\\n • Demonstrated track record of working with consumer brand initiatives or enhancing customer experiences through data\\\\-driven strategies.\\n \\n\\n  \\n\\nSpecialized Training or Knowledge: \\n\\n • Proficiency with Microsoft Office applications, with expertise in Excel (e.g., pivot tables, advanced functions, formulas, filtering, etc.) and database skills (e.g., SQL)'),\n",
       " Document(id='ccdab826-ca4c-4a9e-9f84-0c93ea526826', metadata={'job_title': 'Data Analyst', 'company': 'Jaipur Rugs', 'location': 'Jaipur, Rajasthan, India', 'searched_city': 'Jaipur', 'language': 'en'}, page_content=\"• Legacy: Comfortable with creating foundational data strategy that will enable a hyper growth strategy.\\n \\n\\n • Data Governance: Ensure data integrity and consistency across all reporting, documentation of data assets and KPIs, establish source of truth for data domains. Implement data quality standards and procedures, overseeing data cleansing and validation processes.\\n \\n\\n • Technical Expertise: Utilize SSIS for ETL processes, ensuring seamless data integration and automation across systems. Employ Power BI for comprehensive business intelligence solutions, creating dynamic reports and dashboards that drive decision\\\\-making\\n \\n\\n • Continuous Improvement: Stay informed about industry trends and emerging technologies in data and analytics. Identify opportunities for process optimization and the application of advanced analytics techniques and build a 3\\\\-year roadmap for data analytics.\\n \\n\\n  \\n\\nSkills \\\\& Minimum Qualifications: \\n To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of knowledge, skill, and/or ability required. Reasonable accommodation may be made to enable individuals with disabilities to perform essential functions.\\n \\n\\n • Bachelors or Master's in Data Science, Computer Science, or related fields. • At least 7 years of experience in a leadership role within analytics/data science, specifically within the retail or ecommerce sector.\")]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_sample.invoke(\"what are skills required for data science job role \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store.save_local('job_vector_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\"\"\"You are a friendly and knowledgeable AI career mentor.  Your role is to provide insightful and helpful career\n",
    "        advice to users based on real-world job market data. You will use information on job skills, and required qualifications to \n",
    "        provide assistance. If you don't know the answer, provide alternative options and be honest about what you don't know.\n",
    "\n",
    "        Instructions:\n",
    "        1.  Carefully analyze the context provided, which contains relevant job descriptions and extracted skills.\n",
    "        2.  Based on the context, answer the user's question in a clear, concise, and easy-to-understand manner.\n",
    "        3.  If the query cannot be accurately answered based on the context, admit that you lack sufficient information and suggest \n",
    "        rephrasing the query or providing more details.\n",
    "        4.  Avoid making up information or providing speculative answers.\n",
    "\n",
    "        Context: {context}\n",
    "        \"\"\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',system_prompt), \n",
    "        ('human',\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain=create_stuff_documents_chain(llm,prompt1)\n",
    "retriever=vectore_store.as_retriever(search_type='similarity',search_kwargs={'k':3})\n",
    "retriever_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2='How can I transition from a data analyst to a data scientist role?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retriever_chain.invoke({'input':inputs2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitioning from a data analyst to a data scientist role requires a combination of skills, experience, and dedication. Based on the job brief provided, here are some key takeaways to help you make this transition:\n",
      "\n",
      "1. **Develop advanced analytical skills**: As a data analyst, you likely have a strong foundation in data analysis. To become a data scientist, you'll need to develop skills in machine learning, programming languages like Python, R, or SQL, and experience with data manipulation and visualization tools.\n",
      "2. **Gain experience with machine learning techniques**: The job brief mentions specific machine learning algorithms like Classification, Regression, Clustering, and Decision Trees. Familiarize yourself with these techniques and practice implementing them using popular libraries like scikit-learn or TensorFlow.\n",
      "3. **Improve your programming skills**: Data scientists need to be proficient in programming languages like Python, R, or SQL. Focus on developing your skills in one or more of these languages, and learn to work with popular data science libraries and frameworks.\n",
      "4. **Learn data governance and data quality**: As a data scientist, you'll be responsible for ensuring data integrity and consistency. Study data governance principles, data quality standards, and learn to implement data validation and cleansing processes.\n",
      "5. **Stay up-to-date with industry trends and emerging technologies**: The job brief mentions the importance of staying informed about industry trends and emerging technologies in data and analytics. Attend conferences, read industry blogs, and participate in online forums to stay current.\n",
      "6. **Develop strong communication and project management skills**: As a data scientist, you'll need to communicate complex technical concepts to non-technical stakeholders and manage multiple projects effectively. Practice presenting your findings and results to different audiences, and develop your project management skills to prioritize tasks and meet deadlines.\n",
      "7. **Consider pursuing a graduate degree or certifications**: While not necessary, a Master's degree in Data Science, Computer Science, or a related field can be beneficial for advanced roles. Additionally, consider obtaining certifications like Certified Data Scientist (CDS) or Certified Analytics Professional (CAP) to demonstrate your expertise.\n",
      "8. **Network and build a professional portfolio**: Connect with other data professionals, attend industry events, and build a portfolio of your work to showcase your skills and experience to potential employers.\n",
      "\n",
      "In terms of specific skills, focus on developing:\n",
      "\n",
      "* Programming skills in Python, R, or SQL\n",
      "* Experience with machine learning libraries like scikit-learn, TensorFlow, or PyTorch\n",
      "* Data visualization skills using tools like Tableau, Power BI, or D3.js\n",
      "* Data governance and data quality principles\n",
      "* Strong communication and project management skills\n",
      "\n",
      "By following these steps and focusing on developing the necessary skills, you can increase your chances of successfully transitioning from a data analyst to a data scientist role.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about malaria in the provided context, which appears to be a job description for a Consultant, Data Science and Analytics position at TransUnion. The context discusses roles and responsibilities related to mobile app development, team management, and data science, but it does not mention malaria.\n",
      "\n",
      "If you're looking for information about malaria, I suggest searching for it on a reliable health or medical website, such as the World Health Organization (WHO) or the Centers for Disease Control and Prevention (CDC). They should have accurate and up-to-date information about malaria, its causes, symptoms, treatment, and prevention.\n"
     ]
    }
   ],
   "source": [
    "input2='What is malaria?'\n",
    "response2=retriever_chain.invoke({'input':input2})\n",
    "print(response2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt  = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n",
    "    \"Formulate a standalone query which can be understood without the chat history.\"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return as it is.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        ('human',\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the following Python libraries are highly in-demand for Data Science roles:\n",
      "\n",
      "1. **NumPy**: The NumPy library is a fundamental library for numerical computing in Python, and is widely used in Data Science for tasks such as data manipulation and analysis.\n",
      "2. **Pandas**: The Pandas library is a powerful library for data manipulation and analysis, and is widely used in Data Science for tasks such as data cleaning, filtering, and grouping.\n",
      "3. **Matplotlib** and **Seaborn**: These libraries are widely used for data visualization, and are essential for creating informative and engaging visualizations.\n",
      "4. **Scikit-learn**: The Scikit-learn library is a widely used library for machine learning, and provides a wide range of algorithms for tasks such as classification, regression, clustering, and more.\n",
      "5. **TensorFlow** or **PyTorch**: These libraries are widely used for deep learning, and provide a wide range of tools and frameworks for building and training neural networks.\n",
      "6. **Flask** or **FastAPI**: These libraries are widely used for building REST APIs, and are essential for deploying Data Science models in production environments.\n",
      "7. **Scipy**: The Scipy library is a widely used library for scientific computing, and provides a wide range of tools and functions for tasks such as signal processing, linear algebra, and optimization.\n",
      "\n",
      "Additionally, having experience with other libraries such as **Keras**, **OpenCV**, and **NLTK** can also be beneficial for Data Science roles, particularly those involving computer vision, natural language processing, or other specialized areas.\n",
      "\n",
      "It's worth noting that the specific libraries and tools required may vary depending on the organization, industry, or specific job role. However, having a strong foundation in the above-mentioned libraries will provide a solid foundation for a career in Data Science.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What are the key skills I need to become a Data Scientist in the current job market?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"Okay, that's helpful. You mentioned Python. Which specific Python libraries are most in-demand for Data Science roles right now?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What are the key skills I need to become a Data Scientist in the current job market?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Based on the context provided, to become a Data Scientist in the current job market, you should possess the following key skills:\\n\\n1. **Data Management and Analytics**: Working knowledge of data management and analytics platforms such as SQL Server, SSIS, Power BI, and Google Analytics.\\n2. **Statistical Analysis and Machine Learning**: Demonstrated ability in statistical analysis, predictive modeling, and machine learning techniques, including algorithms like Classification, Regression, Clustering, Feature Engineering, Decision Trees, and Gradient Boosting.\\n3. **Data Visualization and Communication**: Ability to communicate trends and insights clearly and concisely to stakeholders at all levels of the organization, using techniques such as data visualization and presentation skills.\\n4. **Programming Skills**: Advanced programming skills, including proficiency in a statistical language like R, and experience with other programming languages like SQL, Hive, Pig, Python, C/C++, and Java.\\n5. **Data Warehouse Management**: Familiarity with warehouse management tools and expertise in utilizing data analytics to optimize supply chain operations and enhance efficiency within warehouse management systems.\\n6. **Project Management**: Strong project management skills, with the ability to manage multiple assignments effectively and lead cross-functional initiatives.\\n7. **Business Acumen**: Ability to analyze and interpret trends in large datasets, identifying patterns and insights that drive strategic decision-making and inform business planning.\\n8. **ERP Systems**: Familiarity with ERP systems and their application in data-driven business environments.\\n9. **Microsoft Office**: Proficiency with Microsoft Office applications, particularly Excel (e.g., pivot tables, advanced functions, formulas, filtering) and database skills (e.g., SQL).\\n10. **Interpersonal and Communication Skills**: Versatile interpersonal and communication style, with the ability to effectively communicate at multiple levels within and outside the organization.\\n\\nAdditionally, having a Bachelor's degree in a quantitative field (e.g., statistics, applied mathematics, financial mathematics, engineering, operations research) and at least 4-6 years of professional experience in analytics or a related field is highly preferred.\\n\\nKeep in mind that the specific requirements may vary depending on the organization, industry, or specific job role. However, possessing these key skills will provide a solid foundation for a career as a Data Scientist in the current job market.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided, to become a Data Scientist in the current job market, you'll need to possess a combination of technical, business, and soft skills. Here are the key skills required:\\n\\n**Technical Skills:**\\n\\n1. **Programming skills**: Proficiency in languages such as Python, R, SQL, and Java.\\n2. **Data management and analytics**: Experience with data management and analytics platforms like SQL Server, SSIS, Power BI, and Google Analytics.\\n3. **Machine learning and statistical analysis**: Knowledge of machine learning techniques, statistical analysis, and predictive modeling.\\n4. **Data visualization**: Familiarity with data visualization tools and techniques to effectively communicate insights.\\n5. **Database skills**: Understanding of database concepts and experience with database management systems.\\n\\n**Business and Quantitative Skills:**\\n\\n1. **Quantitative field**: A degree in a quantitative field such as statistics, applied mathematics, financial mathematics, engineering, or operations research.\\n2. **Data-driven decision making**: Ability to analyze and interpret trends in large datasets to drive strategic decision-making.\\n3. **ERP systems**: Familiarity with Enterprise Resource Planning (ERP) systems and their application in data-driven business environments.\\n4. **Supply chain operations**: Knowledge of optimizing supply chain operations using data analytics.\\n\\n**Soft Skills:**\\n\\n1. **Communication**: Ability to communicate complex trends and insights clearly and concisely to stakeholders at all levels.\\n2. **Project management**: Strong project management skills to manage multiple assignments and lead cross-functional initiatives.\\n3. **Collaboration**: Ability to work in a collaborative, fast-paced environment and effectively communicate with team members.\\n4. **Time management**: Ability to balance multiple projects with competing deadlines.\\n\\n**Preferred Skills:**\\n\\n1. **Advanced programming skills**: Experience with programming languages like C/C++, Hive, Pig, and Python.\\n2. **Machine learning algorithms**: Knowledge of machine learning algorithms such as Classification, Regression, Clustering, Feature Engineering, Decision Trees, and Gradient Boosting.\\n3. **Microsoft Office tools**: Proficiency with Microsoft Office applications, particularly Excel, and database skills like SQL.\\n\\nTo become a successful Data Scientist, focus on developing a strong foundation in technical, business, and soft skills, and stay up-to-date with industry trends and advancements.\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are the key skills I need to become a Data Scientist in the current job market?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What are the key skills I need to become a Data Scientist in the current job market?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Based on the context provided, to become a Data Scientist in the current job market, you'll need to possess a combination of technical, business, and soft skills. Here are the key skills required:\\n\\n**Technical Skills:**\\n\\n1. **Programming skills**: Proficiency in languages such as Python, R, SQL, and Java.\\n2. **Data management and analytics**: Experience with data management and analytics platforms like SQL Server, SSIS, Power BI, and Google Analytics.\\n3. **Machine learning and statistical analysis**: Knowledge of machine learning techniques, statistical analysis, and predictive modeling.\\n4. **Data visualization**: Familiarity with data visualization tools and techniques to effectively communicate insights.\\n5. **Database skills**: Understanding of database concepts and experience with database management systems.\\n\\n**Business and Quantitative Skills:**\\n\\n1. **Quantitative field**: A degree in a quantitative field such as statistics, applied mathematics, financial mathematics, engineering, or operations research.\\n2. **Data-driven decision making**: Ability to analyze and interpret trends in large datasets to drive strategic decision-making.\\n3. **ERP systems**: Familiarity with Enterprise Resource Planning (ERP) systems and their application in data-driven business environments.\\n4. **Supply chain operations**: Knowledge of optimizing supply chain operations using data analytics.\\n\\n**Soft Skills:**\\n\\n1. **Communication**: Ability to communicate complex trends and insights clearly and concisely to stakeholders at all levels.\\n2. **Project management**: Strong project management skills to manage multiple assignments and lead cross-functional initiatives.\\n3. **Collaboration**: Ability to work in a collaborative, fast-paced environment and effectively communicate with team members.\\n4. **Time management**: Ability to balance multiple projects with competing deadlines.\\n\\n**Preferred Skills:**\\n\\n1. **Advanced programming skills**: Experience with programming languages like C/C++, Hive, Pig, and Python.\\n2. **Machine learning algorithms**: Knowledge of machine learning algorithms such as Classification, Regression, Clustering, Feature Engineering, Decision Trees, and Gradient Boosting.\\n3. **Microsoft Office tools**: Proficiency with Microsoft Office applications, particularly Excel, and database skills like SQL.\\n\\nTo become a successful Data Scientist, focus on developing a strong foundation in technical, business, and soft skills, and stay up-to-date with industry trends and advancements.\", additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, the following Python libraries are in high demand for Data Science roles:\\n\\n1. **NumPy**: The NumPy library is a fundamental library for numerical computing in Python, and is widely used in Data Science for tasks such as data manipulation and analysis.\\n2. **Pandas**: The Pandas library is a powerful library for data manipulation and analysis, and is widely used in Data Science for tasks such as data cleaning, filtering, and grouping.\\n3. **Scikit-learn**: The Scikit-learn library is a popular library for machine learning in Python, and is widely used in Data Science for tasks such as classification, regression, clustering, and model selection.\\n4. **TensorFlow** or **PyTorch**: Both TensorFlow and PyTorch are popular deep learning libraries in Python, and are widely used in Data Science for tasks such as building and training neural networks.\\n5. **Matplotlib** and/or **Seaborn**: Both Matplotlib and Seaborn are popular data visualization libraries in Python, and are widely used in Data Science for tasks such as creating plots, charts, and heatmaps.\\n6. **Flask** or **FastAPI**: Both Flask and FastAPI are popular libraries for building REST APIs in Python, and are widely used in Data Science for tasks such as deploying machine learning models and creating data-driven web applications.\\n\\nThese libraries are mentioned in the context as essential skills for the AI/ML Developer role, and are widely used in the industry for Data Science tasks. Having experience with these libraries can be beneficial for a career in Data Science.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Okay, that's helpful. You mentioned Python. Which specific Python libraries are most in-demand for Data Science roles right now?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
