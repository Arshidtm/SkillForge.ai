{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "welcome\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('welcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv('RAPIDAPI_KEY')\n",
    "\n",
    "INDIAN_CITIES = [\n",
    "    \"Mumbai\", \"Delhi\", \"Bangalore\", \"Hyderabad\", \"Ahmedabad\",\n",
    "    \"Chennai\", \"Kolkata\", \"Surat\", \"Pune\", \"Jaipur\",\n",
    "    \"Lucknow\", \"Kanpur\", \"Nagpur\", \"Visakhapatnam\", \"Indore\",\n",
    "    \"Thane\", \"Bhopal\", \"Patna\", \"Vadodara\", \"Ghaziabad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def fetch_jobs(query, location=\"India\", results_wanted=5,api_key=api_key):\n",
    "#     conn = http.client.HTTPSConnection(\"jobs-search-api.p.rapidapi.com\")\n",
    "    \n",
    "#     # If location is \"India\", use random cities\n",
    "#     if location.lower() == \"india\":\n",
    "#         # Calculate how many jobs per city (at least 1 city per job)\n",
    "#         jobs_per_city = max(1, results_wanted // len(INDIAN_CITIES))\n",
    "#         all_jobs = []\n",
    "        \n",
    "#         for city in random.sample(INDIAN_CITIES, min(len(INDIAN_CITIES), results_wanted)):\n",
    "#             payload = json.dumps({\n",
    "#                 \"search_term\": query,\n",
    "#                 \"location\": f\"{city}, India\",\n",
    "#                 \"results_wanted\": jobs_per_city,\n",
    "#                 \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "#                 \"distance\": 50,\n",
    "#                 \"job_type\": \"fulltime\",\n",
    "#                 \"is_remote\": False,\n",
    "#                 \"linkedin_fetch_description\": True,\n",
    "#                 \"hours_old\": 72\n",
    "#             })\n",
    "\n",
    "#             headers = {\n",
    "#     'x-rapidapi-key': \"ab8248b873msh2242a61781bb598p1262a9jsn98ab57e85059\",\n",
    "#     'x-rapidapi-host': \"jobs-search-api.p.rapidapi.com\",\n",
    "#     'Content-Type': \"application/json\"\n",
    "# }\n",
    "\n",
    "#             try:\n",
    "#                 conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "#                 res = conn.getresponse()\n",
    "#                 data = res.read().decode(\"utf-8\")\n",
    "#                 city_jobs = json.loads(data).get(\"jobs\", [])\n",
    "                \n",
    "#                 # Add city information to each job\n",
    "#                 for job in city_jobs:\n",
    "#                     job[\"searched_location\"] = city\n",
    "#                 all_jobs.extend(city_jobs)\n",
    "                \n",
    "#                 # Stop if we've collected enough jobs\n",
    "#                 if len(all_jobs) >= results_wanted:\n",
    "#                     break\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error fetching jobs for {city}: {str(e)}\")\n",
    "#                 continue\n",
    "                \n",
    "#         # Trim to exact result count and format\n",
    "#         return [\n",
    "#             {\n",
    "#                 \"job title\": job[\"title\"],\n",
    "#                 \"company\": job[\"company\"],\n",
    "#                 \"location\": job.get(\"location\", \"N/A\"),\n",
    "#                 \"searched_city\": job.get(\"searched_location\", \"India\"),\n",
    "#                 \"description\": job[\"description\"]\n",
    "#             }\n",
    "#             for job in all_jobs[:results_wanted]\n",
    "#             if all(key in job for key in [\"title\", \"company\", \"description\"])\n",
    "#         ]\n",
    "    \n",
    "#     else:\n",
    "#         # Original single-location logic\n",
    "#         payload = json.dumps({\n",
    "#             \"search_term\": query,\n",
    "#             \"location\": location,\n",
    "#             \"results_wanted\": results_wanted,\n",
    "#             \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "#             \"distance\": 50,\n",
    "#             \"job_type\": \"fulltime\",\n",
    "#             \"is_remote\": False,\n",
    "#             \"linkedin_fetch_description\": True,\n",
    "#             \"hours_old\": 72,\n",
    "#             \"show_requirements\": True, \n",
    "#         })\n",
    "\n",
    "#         headers = {\n",
    "#     'x-rapidapi-key': \"ab8248b873msh2242a61781bb598p1262a9jsn98ab57e85059\",\n",
    "#     'x-rapidapi-host': \"jobs-search-api.p.rapidapi.com\",\n",
    "#     'Content-Type': \"application/json\"\n",
    "# }\n",
    "\n",
    "#         conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "#         res = conn.getresponse()\n",
    "#         data = res.read().decode(\"utf-8\")\n",
    "#         job_data = json.loads(data)\n",
    "\n",
    "#         return [\n",
    "#             {\n",
    "#                 \"job title\": job[\"title\"],\n",
    "#                 \"company\": job[\"company\"],\n",
    "#                 \"location\": job.get(\"location\", \"N/A\"),\n",
    "#                 \"searched_city\": location.split(\",\")[0].strip(),\n",
    "#                 \"description\": job[\"description\"]\n",
    "#             }\n",
    "#             for job in job_data.get(\"jobs\", [])\n",
    "#             if all(key in job for key in [\"title\", \"company\", \"description\"])\n",
    "#         ]\n",
    "        \n",
    "def clean_text(text):\n",
    "    \"\"\"Remove excessive newlines and markdown bold syntax\"\"\"\n",
    "    text = re.sub(r'\\*\\*', '', text)  # Remove **bold** markers\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Replace 3+ newlines with double newlines\n",
    "    return text.strip()\n",
    "\n",
    "def documentation(job_details):\n",
    "    content=[] \n",
    "    for job in job_details: \n",
    "        doc = Document(\n",
    "                page_content=clean_text(job[\"description\"]),\n",
    "                metadata={\n",
    "                    \"job_title\": job[\"job title\"],\n",
    "                    \"company\": job[\"company\"],\n",
    "                    \"location\": job[\"location\"],\n",
    "                    \"searched_city\": job[\"searched_city\"],\n",
    "                    \n",
    "                    \"language\": \"en\"\n",
    "                    }\n",
    "                )\n",
    "        content.append(doc)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_jobs(query, location=\"India\", results_wanted=5, api_key=api_key, strict_matching=True):\n",
    "    \"\"\"\n",
    "    Fetch job descriptions from the API based on query and location with strict role matching.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Job role for searching\n",
    "        location (str, optional): Location to search. Defaults to \"India\".\n",
    "        results_wanted (int, optional): Number of job results desired. Defaults to 5.\n",
    "        api_key (str, optional): Rapid API key for authentication. Defaults to api_key.\n",
    "        strict_matching (bool, optional): Whether to enforce strict job title matching. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of job dictionaries containing title, company, location, and description\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting job search for {query} with strict matching\")\n",
    "    \n",
    "    def is_exact_match(job_title, search_query):\n",
    "        \"\"\"Check if job title closely matches the search query\"\"\"\n",
    "        search_terms = search_query.lower().split()\n",
    "        title_terms = job_title.lower().split()\n",
    "        \n",
    "        # Check if all search terms appear in title (order insensitive)\n",
    "        return all(term in \" \".join(title_terms) for term in search_terms)\n",
    "\n",
    "    conn = http.client.HTTPSConnection(\"jobs-search-api.p.rapidapi.com\")\n",
    "\n",
    "    # If location is \"India\", use random cities\n",
    "    if location.lower() == \"india\":\n",
    "        logger.debug(\"Searching across multiple cities in India\")\n",
    "        jobs_per_city = max(1, results_wanted // len(INDIAN_CITIES))\n",
    "        all_jobs = []\n",
    "\n",
    "        for city in random.sample(INDIAN_CITIES, min(len(INDIAN_CITIES), results_wanted)):\n",
    "            payload = json.dumps({\n",
    "                \"search_term\": query,\n",
    "                \"location\": f\"{city}, India\",\n",
    "                \"results_wanted\": jobs_per_city * 2,  # Fetch extra to account for filtering\n",
    "                \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "                \"distance\": 50,\n",
    "                \"job_type\": \"fulltime\",\n",
    "                \"is_remote\": False,\n",
    "                \"linkedin_fetch_description\": True,\n",
    "                \"hours_old\": 72,\n",
    "            })\n",
    "\n",
    "            headers = {\n",
    "                'x-rapidapi-key': \"16e9af54acmsh7b2b87ecf4470ecp1f7eddjsnbad3724607bc\",\n",
    "                'x-rapidapi-host': \"jobs-search-api.p.rapidapi.com\",\n",
    "                'Content-Type': \"application/json\"\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                logger.debug(f\"Fetching jobs for city: {city}\")\n",
    "                conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "                res = conn.getresponse()\n",
    "                data = res.read().decode(\"utf-8\")\n",
    "                city_jobs = json.loads(data).get(\"jobs\", [])\n",
    "\n",
    "                # Filter jobs for exact matches if strict_matching is True\n",
    "                if strict_matching:\n",
    "                    city_jobs = [job for job in city_jobs \n",
    "                               if is_exact_match(job[\"title\"], query) and \n",
    "                               all(key in job for key in [\"title\", \"company\", \"description\"])]\n",
    "\n",
    "                # Add city information to each job\n",
    "                for job in city_jobs:\n",
    "                    job[\"searched_location\"] = city\n",
    "                \n",
    "                all_jobs.extend(city_jobs)\n",
    "\n",
    "                # Stop if we've collected enough jobs\n",
    "                if len(all_jobs) >= results_wanted:\n",
    "                    logger.debug(f\"Reached desired number of jobs: {results_wanted}\")\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching jobs for {city}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Found {len(all_jobs)} matching jobs across Indian cities\")\n",
    "        return [{\n",
    "            \"job title\": job[\"title\"],\n",
    "            \"company\": job[\"company\"],\n",
    "            \"location\": job.get(\"location\", \"N/A\"),\n",
    "            \"searched_city\": job.get(\"searched_location\", \"India\"),\n",
    "            \"description\": job[\"description\"],\n",
    "        } for job in all_jobs[:results_wanted]]\n",
    "\n",
    "    else:\n",
    "        # Original single-location logic with strict matching\n",
    "        logger.debug(f\"Searching in specific location: {location}\")\n",
    "        payload = json.dumps({\n",
    "            \"search_term\": query,\n",
    "            \"location\": location,\n",
    "            \"results_wanted\": results_wanted * 2,  # Fetch extra to account for filtering\n",
    "            \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "            \"distance\": 50,\n",
    "            \"job_type\": \"fulltime\",\n",
    "            \"is_remote\": False,\n",
    "            \"linkedin_fetch_description\": True,\n",
    "            \"hours_old\": 72,\n",
    "            \"show_requirements\": True,\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'x-rapidapi-key': \"16e9af54acmsh7b2b87ecf4470ecp1f7eddjsnbad3724607bc\",\n",
    "            'x-rapidapi-host': \"jobs-search-api.p.rapidapi.com\",\n",
    "            'Content-Type': \"application/json\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read().decode(\"utf-8\")\n",
    "            job_data = json.loads(data)\n",
    "            \n",
    "            jobs = job_data.get(\"jobs\", [])\n",
    "            \n",
    "            # Apply strict matching filter if enabled\n",
    "            if strict_matching:\n",
    "                jobs = [job for job in jobs \n",
    "                       if is_exact_match(job[\"title\"], query) and \n",
    "                       all(key in job for key in [\"title\", \"company\", \"description\"])]\n",
    "\n",
    "            return [{\n",
    "                \"job title\": job[\"title\"],\n",
    "                \"company\": job[\"company\"],\n",
    "                \"location\": job.get(\"location\", \"N/A\"),\n",
    "                \"searched_city\": location.split(\",\")[0].strip(),\n",
    "                \"description\": job[\"description\"],\n",
    "            } for job in jobs[:results_wanted]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching job: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"Data Analyst\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 12:22:56,595 - __main__ - INFO - Starting job search for Data Analyst with strict matching\n",
      "2025-04-14 12:23:42,312 - __main__ - INFO - Found 3 matching jobs across Indian cities\n"
     ]
    }
   ],
   "source": [
    "fetch_job21=fetch_jobs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_job21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=documentation(fetch_job21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'job_title': 'Python Data Analyst', 'company': 'UST', 'location': 'Kolkata, West Bengal, India', 'searched_city': 'Kolkata', 'language': 'en'}, page_content='Role Description\\n We are seeking a skilled\\n Data Analyst \\n with 4\\\\-6 years of experience to join our team. The ideal candidate will have strong expertise in Data analysis,\\n SQL \\n , Data Visualization coupled with excellent analytical and data validation skills. A solid grasp of statistical techniques and the ability to extract actionable insights from complex datasets is essential.\\n   \\n\\n  \\n\\nLocation \\\\- Pune or Kolkata\\nKey Responsibilities \\n  \\n\\n  \\n\\n* Data Pipeline Development:\\n+ Design and implement robust data pipelines for data ingestion, cleansing, and transformation.\\n\\n* Data Extraction and Reporting:\\n+ Extract relevant information from multiple data sources and create detailed reports to support stakeholder decision\\\\-making.\\n\\n* Cross\\\\-Functional Collaboration:\\n+ Collaborate with cross\\\\-functional teams to understand business goals and translate them into analytical solutions.\\n\\n* Statistical Analysis:\\n+ Utilize statistical techniques to analyze data, identifying patterns and trends for strategic decision\\\\-making.\\n\\n* Exploratory Data Analysis (EDA):\\n+ Perform exploratory data analysis to uncover insights and generate hypotheses for further investigation.\\n\\n* Data Visualization:\\n+ Develop and present clear, impactful visualizations and reports to communicate findings and recommendations effectively.\\n\\n* Management Reporting:\\n+ Create comprehensive management reports that highlight trends, patterns, and actionable predictions based on data.\\n\\nPrimary Skills\\n* Good knowledge in Python or PySpark.\\n* Strong SQL skills for data analysis and troubleshooting.\\n* Advanced data analysis and validation expertise.\\n* Proficiency in creating clear and actionable data visualizations.\\n\\nSecondary Skills (Good To Have)\\n* Experience with MS Excel.\\n* Familiarity with Databricks for advanced data processing and collaboration.\\n\\nQualifications\\n* 4\\\\-6 years of experience in data analysis or a related field.\\n* Strong problem\\\\-solving and troubleshooting skills using SQL.\\n\\nSkills\\n SQL Data Analysis Data Visualization, Python or PySpark'),\n",
       " Document(metadata={'job_title': 'Climate Data Analyst', 'company': 'Technip Energies', 'location': 'Uttar Pradesh, India', 'searched_city': 'Lucknow', 'language': 'en'}, page_content='Job Description\\n Be part of the solution at Technip Energies and embark on a one\\\\-of\\\\-a\\\\-kind journey. You will be helping to develop cutting\\\\-edge solutions to solve real\\\\-world energy problems.\\n   \\n\\n  \\n\\n We are currently seeking a Climate Data Analyst, reporting directly to\\n *\\\\[reporting line position]* \\n to join our team based in Noida.\\n   \\n\\n  \\n\\n The Climate Data Analyst will be responsible for collecting, analyzing, and reporting data related to the company’s Greenhouse Gas emissions (scope 3\\\\). They will play a crucial role in supporting the development and implementation of Climate Transition strategies and action plans by providing insights derived from data analysis.\\n   \\n\\n  \\n\\nAbout us:\\n Technip Energies is a global technology and engineering powerhouse. With leadership positions in LNG, hydrogen, ethylene, sustainable chemistry, and CO2 management, we are contributing to the development of critical markets such as energy, energy derivatives, decarbonization, and circularity. Our complementary business segments, Technology, Products and Services (TPS) and Project Delivery, turn innovation into scalable and industrial reality.\\n   \\n\\n  \\n\\n Through collaboration and excellence in execution, our 17,000\\\\+ employees across 34 countries are fully committed to bridging prosperity with sustainability for a world designed to last.\\n   \\n\\n  \\n\\nAbout the opportunity we offer:\\n The Climate Data Analyst will be responsible for collecting, analyzing, and reporting data related to the company’s Greenhouse Gas emissions (scope 3\\\\). They will play a crucial role in supporting the development and implementation of Climate Transition strategies and action plans by providing insights derived from data analysis.\\n   \\n\\n  \\n\\nJob Accountabilities:\\n* Carbon calculation (for scope 3 emissions)\\n* Maintain and improve Carbon calculation tools\\n* Ensure data accuracy, consistency, and integrity through regular audits as part of Data Quality run process.\\n* Train and support projects team in calculating the carbon footprint on projects\\n* Maintain Emission factors to be up\\\\-to\\\\-date\\n* Ensure proper collection of the data from carbon calculation\\n* Analyze data to identify trends, patterns, and insights related to GHG emissions reductions.\\n* Prepare reports, dashboards and presentations to communicate GHG emissions performance.\\n* Climate scenario analysis\\n* Improve the Climate scenario analysis tool\\n* Participate in elaborating different scenarios of GHG emissions reduction\\n* Collaborate with finance modeling to ensure the alignment of data forecast\\n* Elaborate a tool to monitor GHG emissions reduction\\n* Data Analysis\\n* Data crunching and competitive analysis for the various Climate\\\\-related topics.\\n* Collect and manage climate\\\\-related data from various internal and external sources.\\n* Support the Climate change team to optimize internal processes by using digital tools and artificial intelligence.\\n* Gather data and develop a strategic competitor analysis dashboard and reports.\\n* Collaborate with cross\\\\-functional teams, especially IDS and Digital to identify opportunities and develop solutions for the sustainability department.\\n* Contribute to the continuous improvement of sustainability data management processes and systems.\\n\\nAbout you:\\n Bachelor degree or superior in engineering and/or data sciences\\n   \\n\\n  \\n\\n* 4\\\\+ years of relevant work experience\\n* Mastery of Microsoft Tools (PowerPoint, Word, Excel)\\n* Hands\\\\-on experience with data visualization tools (PowerBI preferably)\\n* Knowledge of Python programming and databases\\n* Knowledge of artificial intelligence and machine learning\\n* Excellent data analysis and organizational skills\\n* Strong analytical and data management skills\\n* Ability to work independently and manage multiple projects simultaneously\\n* Knowledge of sustainability issues and commitment to driving sustainable change is a differentiator\\n* Comfortable with complex ecosystem, moving environment and multi\\\\-stakeholders’ management\\n* Good communication and interpersonal skills to effectively collaborate with various teams\\n* Fluent English level is required with good writing skills\\n\\nWhat’s next?\\n Once receiving your application, our Talent Acquisition professionals will screen and match your profile against the role requirements. We ask for your patience as the team completes the volume of applications with reasonable timeframe. Check your application progress periodically via personal account from created candidate profile during your application.\\n   \\n\\n  \\n\\n We invite you to get to know more about our company by visiting and follow us on LinkedIn, Instagram, Facebook, X and YouTube for company updates.'),\n",
       " Document(metadata={'job_title': 'Data Analyst, Labs', 'company': 'DoorDash', 'location': 'Hyderabad, Telangana, India', 'searched_city': 'Hyderabad', 'language': 'en'}, page_content='Hyderabad, preferred or remote\\nAbout the Team\\n DoorDash Labs is an independent team within DoorDash. We explore robotics and automation to transform last mile logistics in the long term. If you want to work on commercializing autonomy and robotics in a service used by millions of people, then we want to talk to you!\\n   \\n\\n  \\n\\nAbout the Role\\n Come help us redefine last\\\\-mile logistics through robotics, automation, and other advanced technologies! DoorDash Labs is an independent team inside of DoorDash incubating long\\\\-term bets and exploring new technologies that can be applied to transform last\\\\-mile logistics. We believe in the value of advanced technologies and strive to build meaningful applications of next\\\\-generation hardware and software to impact real\\\\-world needs.\\n   \\n\\n  \\n\\n We\\'re looking for a data analyst to help bring autonomous robots and other high\\\\-tech automation products to market, at scale.\\n   \\n\\n  \\n\\nYou\\'re excited about this opportunity because you will…\\n* Own analytics and insights across multiple technical and product areas, ensuring data is structured to support clear measurement of business and product performance.\\n* Define a strategy for commercial, operations, and autonomy insights to drive product improvements and answer critical business questions.\\n* Report in directly with the product management team, and support a world\\\\-class team of engineers and business leaders to bring autonomous solutions to market\\n* Use your skill in understanding and synthesis of data to create high value visualization and dashboards\\n* Act as the primary data analytics partner for stakeholders across engineering, product, and leadership, helping to translate complex business questions into measurable data solutions.\\n\\nWe\\'re excited about you because…\\n* You have a Bachelor\\'s degree in business administration, economics, computer science, management information systems, or related field or equivalent related experience\\n* You have 3\\\\-6 years of experience in data analytics or business analytics\\n* You have experience with data bases in Redshift, Oracle, Trino, Snowflake, or other similar systems\\n* You are a SQL expert, and have experience with multiple data visualization or business intelligence tools. Experience with Sigma Computing is preferred.\\n* You have some experience in python or similar programming languages to build ETL pipelines.\\n* Identifying how data can be structured and leveraged to drive meaningful insights.\\n* You excel at defining meaningful metrics and KPIs, ensuring data is structured and accessible for ongoing reporting, dashboarding, and deeper analytics.\"\\n* You want to own insights across team and technology stack, including supporting product development, operations, and commercialization.\\n* You proactively engage with stakeholders to understand business challenges.\\n\\n Notice to Applicants for Jobs Located in NYC or Remote Jobs Associated With Office in NYC Only\\n   \\n\\n  \\n\\n We use Covey as part of our hiring and/or promotional process for jobs in NYC and certain features may qualify it as an AEDT in NYC. As part of the hiring and/or promotion process, we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound from August 21, 2023, through December 21, 2023, and resumed using Covey Scout for Inbound again on June 29, 2024\\\\.\\n   \\n\\n  \\n\\n The Covey tool has been reviewed by an independent auditor. Results of the audit may be viewed here: Covey\\n   \\n\\n  \\n\\nAbout DoorDash\\n At DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door\\\\-to\\\\-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods.\\n   \\n\\n  \\n\\n DoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We\\'re committed to supporting employees\\' happiness, healthiness, and overall well\\\\-being by providing comprehensive benefits and perks.\\n   \\n\\n  \\n\\nOur Commitment to Diversity and Inclusion\\n We\\'re committed to growing and empowering a more inclusive community within our company, industry, and cities. That\\'s why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel.\\n   \\n\\n  \\n\\n If you need any accommodations, please inform your recruiting contact upon initial connection.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"doc_pickle_sample.pkl\",\"wb\") as f:\n",
    "    pickle.dump(docs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \"\n",
    "for i,doc in enumerate(docs):\n",
    "    text += doc.page_content\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_jd(docs):\n",
    "    mid = len(docs)//2\n",
    "    text = \" \"\n",
    "    for i,doc in enumerate(docs):\n",
    "        text += doc.page_content\n",
    "        if i == mid:\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_from_jd(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9185"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Determine if the following text describes a job role. \n",
    "    Answer strictly 'Yes' or 'No'.\n",
    "    \n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def is_job_role(text):\n",
    "    # Format the prompt with the input text\n",
    "    formatted_prompt = prompt.format(text=text)\n",
    "    \n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_job_role(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1500,chunk_overlap=200)\n",
    "\n",
    "text_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arshi\\Downloads\\Desktop\\Bro-Project\\SkillForge.ai\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding=HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store=FAISS.from_documents(text_chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=embeddings.embed_query(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_sample=vectore_store.as_retriever(search_type='similarity',search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='013abf22-a78a-4004-ac88-a2972a685b24', metadata={'job_title': 'Data Analyst', 'company': 'Jaipur Rugs', 'location': 'Jaipur, Rajasthan, India', 'searched_city': 'Jaipur', 'language': 'en'}, page_content='• Proficiency with Microsoft Office applications, with expertise in Excel (e.g., pivot tables, advanced functions, formulas, filtering, etc.) and database skills (e.g., SQL)\\n \\n\\n • Ability to collect and synthesize information, making it relevant, understandable, and actionable for key stakeholders\\n \\n\\n • Ability to balance multiple projects with competing deadlines\\n \\n\\n • Generate insights that improve the business through linking various data sources\\n \\n\\n • Strong understanding of Analytics and Visualization techniques'),\n",
       " Document(id='dde3b1e9-dc9d-43e1-a53f-a54e1db062f0', metadata={'job_title': 'Data Analyst', 'company': 'Jaipur Rugs', 'location': 'Jaipur, Rajasthan, India', 'searched_city': 'Jaipur', 'language': 'en'}, page_content='• Working knowledge in data management and analytics platforms, such as SQL Server, SSIS, Power BI, and Google Analytics. • Demonstrated ability in statistical analysis, predictive modeling, and machine learning techniques.\\n \\n\\n • Demonstrated ability to analyze and interpret trends in large datasets, identifying patterns and insights that drive strategic decision\\\\-making.\\n \\n\\n • Familiarity with ERP systems and their application in data\\\\-driven business environments.\\n \\n\\n • Experience in implementing tailored solutions to improve customer journey mapping, satisfaction metrics, and brand loyalty.\\n \\n\\n • Proficiency in warehouse management tools, demonstrating expertise in utilizing data analytics to optimize supply chain operations and enhance efficiency within warehouse management systems.\\n \\n\\n • Ability to communicate trends and their implications clearly and concisely to stakeholders at all levels of the organization, informing strategic planning and resource allocation.\\n \\n\\n • Strong project management skills, presentation skills, and a successful track record of leading successful cross\\\\-functional initiatives.\\n \\n\\n • Demonstrated track record of working with consumer brand initiatives or enhancing customer experiences through data\\\\-driven strategies.\\n \\n\\n  \\n\\nSpecialized Training or Knowledge: \\n\\n • Proficiency with Microsoft Office applications, with expertise in Excel (e.g., pivot tables, advanced functions, formulas, filtering, etc.) and database skills (e.g., SQL)'),\n",
       " Document(id='ccdab826-ca4c-4a9e-9f84-0c93ea526826', metadata={'job_title': 'Data Analyst', 'company': 'Jaipur Rugs', 'location': 'Jaipur, Rajasthan, India', 'searched_city': 'Jaipur', 'language': 'en'}, page_content=\"• Legacy: Comfortable with creating foundational data strategy that will enable a hyper growth strategy.\\n \\n\\n • Data Governance: Ensure data integrity and consistency across all reporting, documentation of data assets and KPIs, establish source of truth for data domains. Implement data quality standards and procedures, overseeing data cleansing and validation processes.\\n \\n\\n • Technical Expertise: Utilize SSIS for ETL processes, ensuring seamless data integration and automation across systems. Employ Power BI for comprehensive business intelligence solutions, creating dynamic reports and dashboards that drive decision\\\\-making\\n \\n\\n • Continuous Improvement: Stay informed about industry trends and emerging technologies in data and analytics. Identify opportunities for process optimization and the application of advanced analytics techniques and build a 3\\\\-year roadmap for data analytics.\\n \\n\\n  \\n\\nSkills \\\\& Minimum Qualifications: \\n To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of knowledge, skill, and/or ability required. Reasonable accommodation may be made to enable individuals with disabilities to perform essential functions.\\n \\n\\n • Bachelors or Master's in Data Science, Computer Science, or related fields. • At least 7 years of experience in a leadership role within analytics/data science, specifically within the retail or ecommerce sector.\")]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_sample.invoke(\"what are skills required for data science job role \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store.save_local('job_vector_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\"\"\"You are a friendly and knowledgeable AI career mentor.  Your role is to provide insightful and helpful career\n",
    "        advice to users based on real-world job market data. You will use information on job skills, and required qualifications to \n",
    "        provide assistance. If you don't know the answer, provide alternative options and be honest about what you don't know.\n",
    "\n",
    "        Instructions:\n",
    "        1.  Carefully analyze the context provided, which contains relevant job descriptions and extracted skills.\n",
    "        2.  Based on the context, answer the user's question in a clear, concise, and easy-to-understand manner.\n",
    "        3.  If the query cannot be accurately answered based on the context, admit that you lack sufficient information and suggest \n",
    "        rephrasing the query or providing more details.\n",
    "        4.  Avoid making up information or providing speculative answers.\n",
    "\n",
    "        Context: {context}\n",
    "        \"\"\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',system_prompt), \n",
    "        ('human',\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain=create_stuff_documents_chain(llm,prompt1)\n",
    "retriever=vectore_store.as_retriever(search_type='similarity',search_kwargs={'k':3})\n",
    "retriever_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2='How can I transition from a data analyst to a data scientist role?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retriever_chain.invoke({'input':inputs2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitioning from a data analyst to a data scientist role requires a combination of skills, experience, and dedication. Based on the job brief provided, here are some key takeaways to help you make this transition:\n",
      "\n",
      "1. **Develop advanced analytical skills**: As a data analyst, you likely have a strong foundation in data analysis. To become a data scientist, you'll need to develop skills in machine learning, programming languages like Python, R, or SQL, and experience with data manipulation and visualization tools.\n",
      "2. **Gain experience with machine learning techniques**: The job brief mentions specific machine learning algorithms like Classification, Regression, Clustering, and Decision Trees. Familiarize yourself with these techniques and practice implementing them using popular libraries like scikit-learn or TensorFlow.\n",
      "3. **Improve your programming skills**: Data scientists need to be proficient in programming languages like Python, R, or SQL. Focus on developing your skills in one or more of these languages, and learn to work with popular data science libraries and frameworks.\n",
      "4. **Learn data governance and data quality**: As a data scientist, you'll be responsible for ensuring data integrity and consistency. Study data governance principles, data quality standards, and learn to implement data validation and cleansing processes.\n",
      "5. **Stay up-to-date with industry trends and emerging technologies**: The job brief mentions the importance of staying informed about industry trends and emerging technologies in data and analytics. Attend conferences, read industry blogs, and participate in online forums to stay current.\n",
      "6. **Develop strong communication and project management skills**: As a data scientist, you'll need to communicate complex technical concepts to non-technical stakeholders and manage multiple projects effectively. Practice presenting your findings and results to different audiences, and develop your project management skills to prioritize tasks and meet deadlines.\n",
      "7. **Consider pursuing a graduate degree or certifications**: While not necessary, a Master's degree in Data Science, Computer Science, or a related field can be beneficial for advanced roles. Additionally, consider obtaining certifications like Certified Data Scientist (CDS) or Certified Analytics Professional (CAP) to demonstrate your expertise.\n",
      "8. **Network and build a professional portfolio**: Connect with other data professionals, attend industry events, and build a portfolio of your work to showcase your skills and experience to potential employers.\n",
      "\n",
      "In terms of specific skills, focus on developing:\n",
      "\n",
      "* Programming skills in Python, R, or SQL\n",
      "* Experience with machine learning libraries like scikit-learn, TensorFlow, or PyTorch\n",
      "* Data visualization skills using tools like Tableau, Power BI, or D3.js\n",
      "* Data governance and data quality principles\n",
      "* Strong communication and project management skills\n",
      "\n",
      "By following these steps and focusing on developing the necessary skills, you can increase your chances of successfully transitioning from a data analyst to a data scientist role.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about malaria in the provided context, which appears to be a job description for a Consultant, Data Science and Analytics position at TransUnion. The context discusses roles and responsibilities related to mobile app development, team management, and data science, but it does not mention malaria.\n",
      "\n",
      "If you're looking for information about malaria, I suggest searching for it on a reliable health or medical website, such as the World Health Organization (WHO) or the Centers for Disease Control and Prevention (CDC). They should have accurate and up-to-date information about malaria, its causes, symptoms, treatment, and prevention.\n"
     ]
    }
   ],
   "source": [
    "input2='What is malaria?'\n",
    "response2=retriever_chain.invoke({'input':input2})\n",
    "print(response2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt  = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n",
    "    \"Formulate a standalone query which can be understood without the chat history.\"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return as it is.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        ('human',\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the following Python libraries are highly in-demand for Data Science roles:\n",
      "\n",
      "1. **NumPy**: The NumPy library is a fundamental library for numerical computing in Python, and is widely used in Data Science for tasks such as data manipulation and analysis.\n",
      "2. **Pandas**: The Pandas library is a powerful library for data manipulation and analysis, and is widely used in Data Science for tasks such as data cleaning, filtering, and grouping.\n",
      "3. **Matplotlib** and **Seaborn**: These libraries are widely used for data visualization, and are essential for creating informative and engaging visualizations.\n",
      "4. **Scikit-learn**: The Scikit-learn library is a widely used library for machine learning, and provides a wide range of algorithms for tasks such as classification, regression, clustering, and more.\n",
      "5. **TensorFlow** or **PyTorch**: These libraries are widely used for deep learning, and provide a wide range of tools and frameworks for building and training neural networks.\n",
      "6. **Flask** or **FastAPI**: These libraries are widely used for building REST APIs, and are essential for deploying Data Science models in production environments.\n",
      "7. **Scipy**: The Scipy library is a widely used library for scientific computing, and provides a wide range of tools and functions for tasks such as signal processing, linear algebra, and optimization.\n",
      "\n",
      "Additionally, having experience with other libraries such as **Keras**, **OpenCV**, and **NLTK** can also be beneficial for Data Science roles, particularly those involving computer vision, natural language processing, or other specialized areas.\n",
      "\n",
      "It's worth noting that the specific libraries and tools required may vary depending on the organization, industry, or specific job role. However, having a strong foundation in the above-mentioned libraries will provide a solid foundation for a career in Data Science.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What are the key skills I need to become a Data Scientist in the current job market?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"Okay, that's helpful. You mentioned Python. Which specific Python libraries are most in-demand for Data Science roles right now?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What are the key skills I need to become a Data Scientist in the current job market?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Based on the context provided, to become a Data Scientist in the current job market, you should possess the following key skills:\\n\\n1. **Data Management and Analytics**: Working knowledge of data management and analytics platforms such as SQL Server, SSIS, Power BI, and Google Analytics.\\n2. **Statistical Analysis and Machine Learning**: Demonstrated ability in statistical analysis, predictive modeling, and machine learning techniques, including algorithms like Classification, Regression, Clustering, Feature Engineering, Decision Trees, and Gradient Boosting.\\n3. **Data Visualization and Communication**: Ability to communicate trends and insights clearly and concisely to stakeholders at all levels of the organization, using techniques such as data visualization and presentation skills.\\n4. **Programming Skills**: Advanced programming skills, including proficiency in a statistical language like R, and experience with other programming languages like SQL, Hive, Pig, Python, C/C++, and Java.\\n5. **Data Warehouse Management**: Familiarity with warehouse management tools and expertise in utilizing data analytics to optimize supply chain operations and enhance efficiency within warehouse management systems.\\n6. **Project Management**: Strong project management skills, with the ability to manage multiple assignments effectively and lead cross-functional initiatives.\\n7. **Business Acumen**: Ability to analyze and interpret trends in large datasets, identifying patterns and insights that drive strategic decision-making and inform business planning.\\n8. **ERP Systems**: Familiarity with ERP systems and their application in data-driven business environments.\\n9. **Microsoft Office**: Proficiency with Microsoft Office applications, particularly Excel (e.g., pivot tables, advanced functions, formulas, filtering) and database skills (e.g., SQL).\\n10. **Interpersonal and Communication Skills**: Versatile interpersonal and communication style, with the ability to effectively communicate at multiple levels within and outside the organization.\\n\\nAdditionally, having a Bachelor's degree in a quantitative field (e.g., statistics, applied mathematics, financial mathematics, engineering, operations research) and at least 4-6 years of professional experience in analytics or a related field is highly preferred.\\n\\nKeep in mind that the specific requirements may vary depending on the organization, industry, or specific job role. However, possessing these key skills will provide a solid foundation for a career as a Data Scientist in the current job market.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided, to become a Data Scientist in the current job market, you'll need to possess a combination of technical, business, and soft skills. Here are the key skills required:\\n\\n**Technical Skills:**\\n\\n1. **Programming skills**: Proficiency in languages such as Python, R, SQL, and Java.\\n2. **Data management and analytics**: Experience with data management and analytics platforms like SQL Server, SSIS, Power BI, and Google Analytics.\\n3. **Machine learning and statistical analysis**: Knowledge of machine learning techniques, statistical analysis, and predictive modeling.\\n4. **Data visualization**: Familiarity with data visualization tools and techniques to effectively communicate insights.\\n5. **Database skills**: Understanding of database concepts and experience with database management systems.\\n\\n**Business and Quantitative Skills:**\\n\\n1. **Quantitative field**: A degree in a quantitative field such as statistics, applied mathematics, financial mathematics, engineering, or operations research.\\n2. **Data-driven decision making**: Ability to analyze and interpret trends in large datasets to drive strategic decision-making.\\n3. **ERP systems**: Familiarity with Enterprise Resource Planning (ERP) systems and their application in data-driven business environments.\\n4. **Supply chain operations**: Knowledge of optimizing supply chain operations using data analytics.\\n\\n**Soft Skills:**\\n\\n1. **Communication**: Ability to communicate complex trends and insights clearly and concisely to stakeholders at all levels.\\n2. **Project management**: Strong project management skills to manage multiple assignments and lead cross-functional initiatives.\\n3. **Collaboration**: Ability to work in a collaborative, fast-paced environment and effectively communicate with team members.\\n4. **Time management**: Ability to balance multiple projects with competing deadlines.\\n\\n**Preferred Skills:**\\n\\n1. **Advanced programming skills**: Experience with programming languages like C/C++, Hive, Pig, and Python.\\n2. **Machine learning algorithms**: Knowledge of machine learning algorithms such as Classification, Regression, Clustering, Feature Engineering, Decision Trees, and Gradient Boosting.\\n3. **Microsoft Office tools**: Proficiency with Microsoft Office applications, particularly Excel, and database skills like SQL.\\n\\nTo become a successful Data Scientist, focus on developing a strong foundation in technical, business, and soft skills, and stay up-to-date with industry trends and advancements.\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are the key skills I need to become a Data Scientist in the current job market?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What are the key skills I need to become a Data Scientist in the current job market?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Based on the context provided, to become a Data Scientist in the current job market, you'll need to possess a combination of technical, business, and soft skills. Here are the key skills required:\\n\\n**Technical Skills:**\\n\\n1. **Programming skills**: Proficiency in languages such as Python, R, SQL, and Java.\\n2. **Data management and analytics**: Experience with data management and analytics platforms like SQL Server, SSIS, Power BI, and Google Analytics.\\n3. **Machine learning and statistical analysis**: Knowledge of machine learning techniques, statistical analysis, and predictive modeling.\\n4. **Data visualization**: Familiarity with data visualization tools and techniques to effectively communicate insights.\\n5. **Database skills**: Understanding of database concepts and experience with database management systems.\\n\\n**Business and Quantitative Skills:**\\n\\n1. **Quantitative field**: A degree in a quantitative field such as statistics, applied mathematics, financial mathematics, engineering, or operations research.\\n2. **Data-driven decision making**: Ability to analyze and interpret trends in large datasets to drive strategic decision-making.\\n3. **ERP systems**: Familiarity with Enterprise Resource Planning (ERP) systems and their application in data-driven business environments.\\n4. **Supply chain operations**: Knowledge of optimizing supply chain operations using data analytics.\\n\\n**Soft Skills:**\\n\\n1. **Communication**: Ability to communicate complex trends and insights clearly and concisely to stakeholders at all levels.\\n2. **Project management**: Strong project management skills to manage multiple assignments and lead cross-functional initiatives.\\n3. **Collaboration**: Ability to work in a collaborative, fast-paced environment and effectively communicate with team members.\\n4. **Time management**: Ability to balance multiple projects with competing deadlines.\\n\\n**Preferred Skills:**\\n\\n1. **Advanced programming skills**: Experience with programming languages like C/C++, Hive, Pig, and Python.\\n2. **Machine learning algorithms**: Knowledge of machine learning algorithms such as Classification, Regression, Clustering, Feature Engineering, Decision Trees, and Gradient Boosting.\\n3. **Microsoft Office tools**: Proficiency with Microsoft Office applications, particularly Excel, and database skills like SQL.\\n\\nTo become a successful Data Scientist, focus on developing a strong foundation in technical, business, and soft skills, and stay up-to-date with industry trends and advancements.\", additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, the following Python libraries are in high demand for Data Science roles:\\n\\n1. **NumPy**: The NumPy library is a fundamental library for numerical computing in Python, and is widely used in Data Science for tasks such as data manipulation and analysis.\\n2. **Pandas**: The Pandas library is a powerful library for data manipulation and analysis, and is widely used in Data Science for tasks such as data cleaning, filtering, and grouping.\\n3. **Scikit-learn**: The Scikit-learn library is a popular library for machine learning in Python, and is widely used in Data Science for tasks such as classification, regression, clustering, and model selection.\\n4. **TensorFlow** or **PyTorch**: Both TensorFlow and PyTorch are popular deep learning libraries in Python, and are widely used in Data Science for tasks such as building and training neural networks.\\n5. **Matplotlib** and/or **Seaborn**: Both Matplotlib and Seaborn are popular data visualization libraries in Python, and are widely used in Data Science for tasks such as creating plots, charts, and heatmaps.\\n6. **Flask** or **FastAPI**: Both Flask and FastAPI are popular libraries for building REST APIs in Python, and are widely used in Data Science for tasks such as deploying machine learning models and creating data-driven web applications.\\n\\nThese libraries are mentioned in the context as essential skills for the AI/ML Developer role, and are widely used in the industry for Data Science tasks. Having experience with these libraries can be beneficial for a career in Data Science.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Okay, that's helpful. You mentioned Python. Which specific Python libraries are most in-demand for Data Science roles right now?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.getenv('RAPIDAPI_KEY')\n",
    "\n",
    "INDIAN_CITIES = [\n",
    "    \"Mumbai\", \"Delhi\", \"Bangalore\", \"Hyderabad\", \"Ahmedabad\",\n",
    "    \"Chennai\", \"Kolkata\", \"Surat\", \"Pune\", \"Jaipur\",\n",
    "    \"Lucknow\", \"Kanpur\", \"Nagpur\", \"Visakhapatnam\", \"Indore\",\n",
    "    \"Thane\", \"Bhopal\", \"Patna\", \"Vadodara\", \"Ghaziabad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_jobs(query, location=\"India\", results_wanted=5, api_key=api_key, strict_matching=True):\n",
    "    \"\"\"\n",
    "    Fetch job descriptions from the API based on query and location with strict role matching.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Job role for searching\n",
    "        location (str, optional): Location to search. Defaults to \"India\".\n",
    "        results_wanted (int, optional): Number of job results desired. Defaults to 5.\n",
    "        api_key (str, optional): Rapid API key for authentication. Defaults to api_key.\n",
    "        strict_matching (bool, optional): Whether to enforce strict job title matching. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of job dictionaries containing title, company, location, and description\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting job search for {query} with strict matching\")\n",
    "    \n",
    "    def is_exact_match(job_title, search_query):\n",
    "        \"\"\"Check if job title closely matches the search query\"\"\"\n",
    "        search_terms = search_query.lower().split()\n",
    "        title_terms = job_title.lower().split()\n",
    "        \n",
    "        # Check if all search terms appear in title (order insensitive)\n",
    "        return all(term in \" \".join(title_terms) for term in search_terms)\n",
    "\n",
    "    conn = http.client.HTTPSConnection(\"jobs-search-api.p.rapidapi.com\")\n",
    "\n",
    "    # If location is \"India\", use random cities\n",
    "    if location.lower() == \"india\":\n",
    "        logger.debug(\"Searching across multiple cities in India\")\n",
    "        jobs_per_city = max(1, results_wanted // len(INDIAN_CITIES))\n",
    "        all_jobs = []\n",
    "\n",
    "        for city in random.sample(INDIAN_CITIES, min(len(INDIAN_CITIES), results_wanted)):\n",
    "            payload = json.dumps({\n",
    "                \"search_term\": query,\n",
    "                \"location\": f\"{city}, India\",\n",
    "                \"results_wanted\": jobs_per_city * 2,  # Fetch extra to account for filtering\n",
    "                \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "                \"distance\": 50,\n",
    "                \"job_type\": \"fulltime\",\n",
    "                \"is_remote\": False,\n",
    "                \"linkedin_fetch_description\": True,\n",
    "                \"hours_old\": 72,\n",
    "            })\n",
    "\n",
    "            headers = {\n",
    "                \"x-rapidapi-key\": api_key,\n",
    "                \"x-rapidapi-host\": \"jobs-search-api.p.rapidapi.com\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                logger.debug(f\"Fetching jobs for city: {city}\")\n",
    "                conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "                res = conn.getresponse()\n",
    "                data = res.read().decode(\"utf-8\")\n",
    "                city_jobs = json.loads(data).get(\"jobs\", [])\n",
    "\n",
    "                # Filter jobs for exact matches if strict_matching is True\n",
    "                if strict_matching:\n",
    "                    city_jobs = [job for job in city_jobs \n",
    "                               if is_exact_match(job[\"title\"], query) and \n",
    "                               all(key in job for key in [\"title\", \"company\", \"description\"])]\n",
    "\n",
    "                # Add city information to each job\n",
    "                for job in city_jobs:\n",
    "                    job[\"searched_location\"] = city\n",
    "                \n",
    "                all_jobs.extend(city_jobs)\n",
    "\n",
    "                # Stop if we've collected enough jobs\n",
    "                if len(all_jobs) >= results_wanted:\n",
    "                    logger.debug(f\"Reached desired number of jobs: {results_wanted}\")\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching jobs for {city}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Found {len(all_jobs)} matching jobs across Indian cities\")\n",
    "        return [{\n",
    "            \"job title\": job[\"title\"],\n",
    "            \"company\": job[\"company\"],\n",
    "            \"location\": job.get(\"location\", \"N/A\"),\n",
    "            \"searched_city\": job.get(\"searched_location\", \"India\"),\n",
    "            \"description\": job[\"description\"],\n",
    "        } for job in all_jobs[:results_wanted]]\n",
    "\n",
    "    else:\n",
    "        # Original single-location logic with strict matching\n",
    "        logger.debug(f\"Searching in specific location: {location}\")\n",
    "        payload = json.dumps({\n",
    "            \"search_term\": query,\n",
    "            \"location\": location,\n",
    "            \"results_wanted\": results_wanted * 2,  # Fetch extra to account for filtering\n",
    "            \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "            \"distance\": 50,\n",
    "            \"job_type\": \"fulltime\",\n",
    "            \"is_remote\": False,\n",
    "            \"linkedin_fetch_description\": True,\n",
    "            \"hours_old\": 72,\n",
    "            \"show_requirements\": True,\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            \"x-rapidapi-key\": api_key,\n",
    "            \"x-rapidapi-host\": \"jobs-search-api.p.rapidapi.com\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            conn.request(\"POST\", \"/getjobs\", body=payload, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read().decode(\"utf-8\")\n",
    "            job_data = json.loads(data)\n",
    "            \n",
    "            jobs = job_data.get(\"jobs\", [])\n",
    "            \n",
    "            # Apply strict matching filter if enabled\n",
    "            if strict_matching:\n",
    "                jobs = [job for job in jobs \n",
    "                       if is_exact_match(job[\"title\"], query) and \n",
    "                       all(key in job for key in [\"title\", \"company\", \"description\"])]\n",
    "\n",
    "            return [{\n",
    "                \"job title\": job[\"title\"],\n",
    "                \"company\": job[\"company\"],\n",
    "                \"location\": job.get(\"location\", \"N/A\"),\n",
    "                \"searched_city\": location.split(\",\")[0].strip(),\n",
    "                \"description\": job[\"description\"],\n",
    "            } for job in jobs[:results_wanted]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching job: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 19:57:14,813 - __main__ - INFO - Starting job search for data analyst with strict matching\n",
      "2025-04-09 19:57:45,643 - __main__ - INFO - Found 4 matching jobs across Indian cities\n"
     ]
    }
   ],
   "source": [
    "fetch_job = fetch_jobs(\"data analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'job title': 'Data Analyst - Pharmaceutical', 'company': 'Takeda', 'location': 'Bengaluru, Karnataka, India', 'searched_city': 'Bangalore', 'description': 'By clicking the “Apply” button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda’s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge.\\n   \\n\\n  \\n\\n**Job Description:**\\n**The Future Begins Here:**\\n At Takeda, we are leading digital evolution and global transformation. By building innovative solutions and future\\\\-ready capabilities, we are meeting the need of patients, our people, and the planet.\\n   \\n\\n  \\n\\n Bengaluru, the city, which is India’s epicenter of Innovation, has been selected to be home to Takeda’s recently launched Innovation Capability Center. We invite you to join our digital transformation journey. In this role, you will have the opportunity to boost your skills and become the heart of an innovative engine that is contributing to global impact and improvement.\\n   \\n\\n  \\n\\n**At Takeda’s ICC we Unite in Diversity:**\\n Takeda is committed to creating an inclusive and collaborative workplace, where individuals are recognized for their backgrounds and abilities they bring to our company. We are continuously improving our collaborators journey in Takeda, and we welcome applications from all qualified candidates. Here, you will feel welcomed, respected, and valued as an important contributor to our diverse team.\\n   \\n\\n  \\n\\n**The Opportunity :**\\n As a Data Analyst, you will be responsible for collecting, analyzing, and interpreting complex data sets to provide actionable insights and support strategic initiatives.\\n   \\n\\n  \\n\\n**Responsibilities**\\n* Data Collection and Analysis Gather and extract data from various sources, ensuring data integrity, accuracy, and completeness. Clean, transform, and preprocess data to prepare it for analysis.\\n* Performance Monitoring and Optimization Monitor key performance indicators (KPIs) and track the effectiveness of business processes. Identify areas for improvement, potential bottlenecks, and opportunities for optimization. Collaborate with cross\\\\-functional teams to implement data\\\\-driven strategies and measure their impact.\\n* Ad\\\\-Hoc Analysis Conduct ad\\\\-hoc analysis to answer specific business questions or address immediate needs. Analyze data from different perspectives to provide insights into specific areas of interest or emerging trends. Proactively identify new data sources or approaches to enhance analysis capabilities.\\n* Data Quality Assurance Ensure data accuracy, consistency, and adherence to predefined quality standards. Implement data validation and verification procedures to identify and resolve data quality issues. Work closely with data engineers and stakeholders to improve data collection processes and data governance practices.\\n* Collaborative Projects Collaborate with cross\\\\-functional teams, to support data\\\\-driven initiatives and projects. Provide analytical support, guidance, and expertise throughout the project lifecycle. Communicate effectively with stakeholders to understand their requirements and deliver actionable insights.\\n\\n\\n**Skills and Qualifications :**\\n* Bachelor’s Degree with 3\\\\+ years relevant progressive experiences\\n* Proficiency in data manipulation and analysis using tools such as SQL, Python, R, or similar languages.\\n* Experience with data visualization tools such as Tableau, Power BI, or similar software.\\n* Strong analytical thinking and problem\\\\-solving skills.\\n* Familiarity with statistical concepts and techniques.\\n* Excellent attention to detail and ability to work with large datasets.\\n* Strong communication and presentation skills, with the ability to translate complex findings into clear and actionable insights.\\n* Ability to work independently and collaborate effectively in a team environment.\\n* Strong business acumen and ability to understand and address business requirements through data analysis.\\n\\n\\n**What Takeda Can Offer You :**\\n* Takeda is certified as a Top Employer, not only in India, but also globally. No investment we make pays greater dividends than taking good care of our people.\\n* At Takeda, you take the lead on building and shaping your own career.\\n* Joining the ICC in Bangalore will give you access to high\\\\-end technology, continuous training and a diverse and inclusive network of colleagues who will support your career growth.\\n\\n\\n**Benefits :**\\n It is our priority to provide competitive compensation and a benefit package that bridges your personal life with your professional career. Amongst our benefits are\\n   \\n\\n  \\n\\n Competitive Salary \\\\+ Performance Annual Bonus\\n   \\n\\n  \\n\\n* Flexible work environment, including hybrid working\\n* Comprehensive Healthcare Insurance Plans for self, spouse, and children\\n* Group Term Life Insurance and Group Accident Insurance programs\\n* Health \\\\& Wellness programs\\n* Employee Assistance Program\\n* 3 days of leave every year for Voluntary Service in additional to Humanitarian Leaves\\n* Broad Variety of learning platforms\\n* Diversity, Equity, and Inclusion Programs\\n* Reimbursements – Home Internet \\\\& Mobile Phone\\n* Employee Referral Program\\n* Leaves – Paternity Leave (4 Weeks) , Maternity Leave (up to 26 weeks), Bereavement Leave (5 days)\\n\\n\\n**About ICC in Takeda :**\\n* Takeda is leading a digital revolution. We’re not just transforming our company; we’re improving the lives of millions of patients who rely on our medicines every day.\\n* As an organization, we are committed to our cloud\\\\-driven business transformation and believe the ICCs are the catalysts of change for our global organization.\\n\\n\\n**Locations:**\\n IND \\\\- Bengaluru\\n   \\n\\n  \\n\\n**Worker Type:**\\n Employee\\n   \\n\\n  \\n\\n**Worker Sub\\\\-Type:**\\n Regular\\n   \\n\\n  \\n\\n**Time Type:**\\n Full time'}, {'job title': 'Data Analyst', 'company': 'Infosys', 'location': 'Bengaluru East, Karnataka, India', 'searched_city': 'Bangalore', 'description': \"* \\n* 5\\\\+ years of experience as a Data Analyst or similar role.\\n* Proven track record of collecting, cleaning, analyzing, and interpreting large datasets\\n* Expertise in Pipeline designing and Validation\\n* Expertise in statistical methods, machine learning techniques, and data mining techniques\\n* Proficiency in SQL, Python, PySpark, Looker, Prometheus, Carbon, Clickhouse, Kafka, HDFS and ELK stack (Elasticsearch, Logstash, and Kibana)\\n* Experience with data visualization tools such as Grafana and Looker\\n* Ability to work independently and as part of a team\\n* Problem\\\\-solving and analytical skills to extract meaningful insights from data\\n* Strong business acumen to understand the implications of data findings\\n* Collect, clean, and organize large datasets from various sources\\n* Perform data analysis using statistical methods, machine learning techniques, and data visualization tools\\n* Identify patterns, trends, and anomalies within datasets to uncover insights\\n* Develop and maintain data models to represent the organization's business operations\\n* Create interactive dashboards and reports to communicate data findings to stakeholders\\n* Document data analysis procedures and findings to ensure knowledge transfer\\n* 5\\\\+ years of experience as a Data Analyst or similar role.\\n* Proven track record of collecting, cleaning, analyzing, and interpreting large datasets\\n* Expertise in Pipeline designing and Validation\\n* Expertise in statistical methods, machine learning techniques, and data mining techniques\\n* Proficiency in SQL, Python, PySpark, Looker, Prometheus, Carbon, Clickhouse, Kafka, HDFS and ELK stack (Elasticsearch, Logstash, and Kibana)\\n* Experience with data visualization tools such as Grafana and Looker\\n* Ability to work independently and as part of a team\\n* Problem\\\\-solving and analytical skills to extract meaningful insights from data\\n* Strong business acumen to understand the implications of data findings\"}, {'job title': 'Data Analyst - P&G India', 'company': 'Procter & Gamble', 'location': '', 'searched_city': 'Mumbai', 'description': \"**Job Location**\\n MUMBAI GENERAL OFFICE\\n   \\n\\n  \\n\\n**Job Description**\\n**Job Description Template – Data Analyst**\\n**Overview Of The Job**\\n Data Analyst – P\\\\&G India\\n   \\n\\n  \\n\\n This role reports to Director, India Data Platforms, P\\\\&G\\n   \\n\\n  \\n\\n**About India Data Solutions Team:**\\n We take pride in managing the most\\\\-valuable asset of company in Digital World, called Data. Our vision is to deliver Data as a competitive advantage for India Business, by building unified data platforms, delivering customized BI tools for managers \\\\& empowering insightful business decisions through AI in Data. As a data solutions specialist, you'll be working closely with business stakeholders, collaborating to understand their needs and develop solutions to solve problems in area of supply chain, Sales \\\\& Distribution, Consumer Insights \\\\& Market performance.\\n   \\n\\n  \\n\\n In this role, you'll be constantly learning, staying up to date with industry trends and emerging technologies in data solutions. You'll have the chance to work with a variety of tools and technologies, including big data platforms, machine learning frameworks, and data visualization tools, to build innovative and effective solutions.\\n   \\n\\n  \\n\\n So, if you're excited about the possibilities of data, and eager to make a real impact in the world of business, a career in data solutions might be just what you're looking for. Join us and become a part of the future of digital transformation.\\n   \\n\\n  \\n\\n**Click here to hear from the Functional Leader!**\\n**About P\\\\&G IT:**\\n Digital is at the core of P\\\\&G’s accelerated growth strategy. With this vision, IT in P\\\\&G is deeply embedded into every critical process across business organizations comprising 11\\\\+ category units globally creating impactful value through Transformation, Simplification \\\\& Innovation. IT in P\\\\&G is sub\\\\-divided into teams that engage strongly for revolutionizing the business processes to deliver exceptional value \\\\& growth \\\\- Digital GTM, Digital Manufacturing, Marketing Technologist, Ecommerce, Data Sciences \\\\& Analytics, Data Solutions \\\\& Engineering, Product Supply.\\n   \\n\\n  \\n\\n**Responsibilities:**\\n Develop and maintain Power BI reports and dashboards to meet business requirements. Analytical mindset for understanding business requirement. Design and implement data models, transformations, and calculations in Power BI. Create visually appealing and interactive data visualizations using Power BI \\\\& SQL. Perform data analysis and provide actionable insights to support decision\\\\-making. Conduct thorough testing and quality assurance of Power BI dashboards. Develop and execute test plans, test cases, and test scenarios for Power BI reports and visualizations. Identify and report issues, bugs, or discrepancies found during testing and work with the development team to address them. Validate data sources and connections to ensure data accuracy and consistency in Power BI reports. Collaborate with stakeholders to understand their testing requirements and provide insights and recommendations for dashboard improvements.\\n   \\n\\n  \\n\\n**Requirements:**\\n At min 3\\\\+ years of experience working with Power BI, including report/dashboard development and data modelling. Proficiency in SQL and data manipulation for data extraction and transformation. Strong understanding of data visualization principles and best practices. Basic proficiency in Azure Data Factory, Azure Databricks, ADLS, and Azure SQL Database is a plus.\\n   \\n\\n  \\n\\n Ability to translate business requirements into technical solutions using Power BI. Proficiency in DAX \\\\& latest features of PBI (Calculated Measures, Calculated Groups, PBI Pro/Premium Licenses details etc) to develop the best in class product Excellent analytical and problem\\\\-solving skills to derive insights from data. Strong communication and collaboration skills to work effectively with stakeholders. Experience in testing methodologies and best practices for Power BI. Detail\\\\-oriented mindset with a focus on data accuracy and quality. Familiarity with data warehousing concepts and dimensional data modelling. Relevant certifications in Power BI (like Microsoft PBI Associate) or data analytics are advantageous.\\n   \\n\\n  \\n\\n**About Us**\\n We produce globally recognized brands, and we grow the best business leaders in the industry. With a portfolio of trusted brands as diverse as ours, it is paramount our leaders are able to lead with courage the vast array of brands, categories and functions. We serve consumers around the world with one of the strongest portfolios of trusted, quality, leadership brands, including Always®, Ariel®, Gillette®, Head \\\\& Shoulders®, Herbal Essences®, Oral\\\\-B®, Pampers®, Pantene®, Tampax® and more. Our community includes operations in approximately 70 countries worldwide.\\n   \\n\\n  \\n\\n Visit http://www.pg.com to know more.\\n   \\n\\n  \\n\\n We are an equal opportunity employer and value diversity at our company. We do not discriminate against individuals on the basis of race, color, gender, age, national origin, religion, sexual orientation, gender identity or expression, marital status, citizenship, disability, HIV/AIDS status, or any other legally protected factor.\\n   \\n\\n  \\n\\n**Job Qualifications**\\n At min 3\\\\+ years of experience working with Power BI, including report/dashboard development and data modelling. Proficiency in SQL and data manipulation for data extraction and transformation. Strong understanding of data visualization principles and best practices. Basic proficiency in Azure Data Factory, Azure Databricks, ADLS, and Azure SQL Database is a plus.\\n   \\n\\n  \\n\\n Ability to translate business requirements into technical solutions using Power BI. Proficiency in DAX \\\\& latest features of PBI (Calculated Measures, Calculated Groups, PBI Pro/Premium Licenses details etc) to develop the best in class product Excellent analytical and problem\\\\-solving skills to derive insights from data. Strong communication and collaboration skills to work effectively with stakeholders. Experience in testing methodologies and best practices for Power BI. Detail\\\\-oriented mindset with a focus on data accuracy and quality. Familiarity with data warehousing concepts and dimensional data modelling. Relevant certifications in Power BI (like Microsoft PBI Associate) or data analytics are advantageous.\\n   \\n\\n  \\n\\n**Job Schedule**\\n Full time\\n   \\n\\n  \\n\\n**Job Number**\\n R000128262\\n   \\n\\n  \\n\\n**Job Segmentation**\\n Experienced Professionals (Job Segmentation)Starting Pay / Salary Range\"}, {'job title': 'Python Data Analyst', 'company': 'MSCI Inc.', 'location': '', 'searched_city': 'Mumbai', 'description': 'Your Team Responsibilities\\n   \\n\\n  \\n\\n The ESG Ratings Operations team member researches and rates companies on a ‘AAA’ to ‘CCC’ scale based on its exposure to industry specific ESG risks and its ability to manage those risks relative to peers. The team analyzes data sourced through internal and external data suppliers and follows a rule\\\\- based methodology to determine the rating. The companies rated are spread across the developed and emerging markets and range in size from large to small scale.\\n   \\n\\n  \\n\\n Your Key Responsibilities\\n   \\n\\n  \\n\\n**Responsibilities**\\n This role combines data analysis, process automation, efficiency\\\\-building, and data quality control to enhance data reliability and drive informed business decisions. Your primary responsibility will be to review and analyze data, manage data quality rules and standards, apply technical solutions to data \\\\& business problems related to quality and process automations, thereby building efficiency in the process with minimal to no error. Your specific responsibilities shall include:\\n   \\n\\n  \\n\\n* Deliver high quality ESG research for companies in our global coverage on a timely basis.\\n* Create, implement, and manage data quality rules and standards.\\n* Hands on experience with python programming (minimum 1 year).\\n* Interact with business and technical stakeholders on a regular basis to clearly understand requirements, seek regular feedback \\\\& sign offs on project implementation.\\n* Contribute to ongoing improvement of ratings model; ability to handle parts of project independently or with minimum assistance.\\n* Keeping stakeholders \\\\& management updated regularly on project status and escalate blockers / issues at the earliest.\\n* Staying updated with latest advancements in technical space so as to relate the projects with real world scenarios.\\n* Contribute to implementing processes critical for ensuring operational stability.\\n\\n\\n**Your Skills And Experience That Will Help You Excel**\\n* Hands on Computer programming experience preferably with Python programming.\\n* Ability to perform Data analysis using Python and Excel.\\n* Awareness about Data visualization techniques using graphs, charts either with Python or Reporting tools like PowerBI.\\n* Strong communication skills \\\\- written \\\\& verbal.\\n* Strong analytical skills and ability to articulate viewpoints.\\n* Comfortable working in a team environment across hierarchies, functions, and geographies; under minimum supervision.\\n* Team player, quick learner with inquisitive mindset willing to challenge status quo and bring fresh ideas to the process of solutioning.\\n\\n\\n**About MSCI**\\n**What we offer you**\\n* Transparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\\n* Flexible working arrangements, advanced technology, and collaborative workspaces.\\n* A culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\\n* A global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\\n* Global Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\\n* Multi\\\\-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\\n* We actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride \\\\& Allies, Women in Tech, and Women’s Leadership Forum.\\n\\n\\n At MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry\\\\-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\\n   \\n\\n  \\n\\n MSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry\\\\-leading research\\\\-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\\n   \\n\\n  \\n\\n MSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e\\\\-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\\n   \\n\\n  \\n\\n**To all recruitment agencies**\\n MSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\\n   \\n\\n  \\n\\n**Note on recruitment scams**\\n We are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com'}]\n"
     ]
    }
   ],
   "source": [
    "# fetch_job = fetch_jobs(\"data analyst\")\n",
    "print(fetch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst - Pharmaceutical\n",
      "Data Analyst\n",
      "Data Analyst - P&G India\n",
      "Python Data Analyst\n"
     ]
    }
   ],
   "source": [
    "for jobs in fetch_job:\n",
    "    print(jobs['job title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
